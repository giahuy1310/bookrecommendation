{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvITKeEzSb4x"
      },
      "source": [
        "# import data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hX49Mwlo-kc"
      },
      "source": [
        "#Using apache spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RVQkOJ6cpJe3",
        "outputId": "165441ba-9827-49ff-a2d2-5b8c70da5544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: findspark in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.0.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: implicit in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.7.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from implicit) (2.2.5)\n",
            "Requirement already satisfied: threadpoolctl in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from implicit) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from implicit) (4.67.1)\n",
            "Requirement already satisfied: scipy>=0.16 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from implicit) (1.15.3)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.2.3)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.2.5)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.10.3)\n",
            "Requirement already satisfied: numpy>=1.23 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (2.2.5)\n",
            "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: seaborn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from seaborn) (3.10.3)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from seaborn) (2.2.5)\n",
            "Requirement already satisfied: pandas>=1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: jupyter in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.1.1)\n",
            "Requirement already satisfied: jupyter-console in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter) (6.6.3)\n",
            "Requirement already satisfied: notebook in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter) (7.4.2)\n",
            "Requirement already satisfied: jupyterlab in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter) (4.4.2)\n",
            "Requirement already satisfied: ipywidgets in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter) (8.1.7)\n",
            "Requirement already satisfied: nbconvert in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from jupyter) (6.29.5)\n",
            "Requirement already satisfied: psutil in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (7.0.0)\n",
            "Requirement already satisfied: comm>=0.1.1 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (0.2.2)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (1.8.14)\n",
            "Requirement already satisfied: nest-asyncio in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (1.6.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (8.6.3)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (8.36.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (5.7.2)\n",
            "Requirement already satisfied: packaging in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (25.0)\n",
            "Requirement already satisfied: tornado>=6.1 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (6.4.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (0.1.7)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (5.14.3)\n",
            "Requirement already satisfied: pyzmq>=24 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (26.4.0)\n",
            "Requirement already satisfied: appnope in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (0.1.4)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipywidgets->jupyter) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipywidgets->jupyter) (3.0.15)\n",
            "Requirement already satisfied: pygments in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from jupyter-console->jupyter) (2.19.1)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.30 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from jupyter-console->jupyter) (3.0.51)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (3.1.6)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.0.5)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.27.3)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.2.5)\n",
            "Requirement already satisfied: tomli>=1.2.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.2.1)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (0.28.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.16.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (0.2.4)\n",
            "Requirement already satisfied: setuptools>=41.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (65.5.0)\n",
            "Requirement already satisfied: nbformat>=5.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (5.10.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (0.3.0)\n",
            "Requirement already satisfied: bleach[css]!=5.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (6.2.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (3.1.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (4.13.4)\n",
            "Requirement already satisfied: defusedxml in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (1.5.1)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (0.10.2)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (3.0.2)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from async-lru>=1.0.0->jupyterlab->jupyter) (4.13.2)\n",
            "Requirement already satisfied: webencodings in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (1.4.0)\n",
            "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.0.9)\n",
            "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (2025.4.26)\n",
            "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter) (0.16.0)\n",
            "Requirement already satisfied: decorator in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.2.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.9.0)\n",
            "Requirement already satisfied: exceptiongroup in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (1.2.2)\n",
            "Requirement already satisfied: stack_data in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.6.3)\n",
            "Requirement already satisfied: jedi>=0.16 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.19.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.3.7)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.3)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (23.1.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.22.0)\n",
            "Requirement already satisfied: jupyter-events>=0.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.12.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.18.1)\n",
            "Requirement already satisfied: overrides>=5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
            "Requirement already satisfied: requests>=2.31 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.3)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.23.0)\n",
            "Requirement already satisfied: babel>=2.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.17.0)\n",
            "Requirement already satisfied: json5>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.12.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert->jupyter) (2.21.1)\n",
            "Requirement already satisfied: wcwidth in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter) (0.2.13)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (21.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (25.3.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.25.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.36.2)\n",
            "Requirement already satisfied: rfc3339-validator in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.3.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter) (0.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->jupyter) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.4.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter) (2.2.0)\n",
            "Requirement already satisfied: pure-eval in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter) (0.2.3)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter) (3.0.0)\n",
            "Requirement already satisfied: isoduration in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (20.11.0)\n",
            "Requirement already satisfied: fqdn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (1.5.1)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.0.0)\n",
            "Requirement already satisfied: uri-template in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (24.11.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.22)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.9.0.20250516)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: wget in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: unzip in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.0.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Download Spark\n",
        "%pip install pyspark\n",
        "%pip install findspark\n",
        "\n",
        "#Install implicit\n",
        "%pip install implicit\n",
        "%pip install pandas\n",
        "%pip install numpy\n",
        "%pip install os\n",
        "%pip install matplotlib\n",
        "%pip install seaborn\n",
        "%pip install jupyter\n",
        "%pip install wget\n",
        "%pip install unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4okrjU1VpazO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "vXOC73WHpa7G"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/05/24 00:36:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Initialize findspark and create SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BookRecommendation\") \\\n",
        "    .config(\"spark.driver.memory\", \"6g\") \\\n",
        "    .config(\"spark.executor.memory\", \"6g\") \\\n",
        "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
        "    .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
        "    .config(\"spark.driver.host\", \"localhost\") \\\n",
        "    .config(\"spark.driver.bindAddress\", \"localhost\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.default.parallelism\", \"200\") \\\n",
        "    .config(\"spark.network.timeout\", \"600s\") \\\n",
        "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
        "    .getOrCreate()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HHN2nBGayqHd"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import count, col, when, mean, rand  # Import all at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BRcHAN5Fd0NI",
        "outputId": "4037c23f-73d8-476a-e833-04f233a221be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data files already exist\n"
          ]
        }
      ],
      "source": [
        "#Check if we have the data\n",
        "# Check if data files exist\n",
        "if os.path.exists('Users.csv') and os.path.exists('Books.csv') and os.path.exists('Ratings.csv'):\n",
        "    print(\"Data files already exist\")\n",
        "    exit\n",
        "else:\n",
        "    # Import data from GitHub using wget\n",
        "    !wget -O data.zip 'https://github.com/giahuy1310/bookrecommendation/raw/main/BookDataset.zip'\n",
        "    !unzip data.zip\n",
        "    # Load data into Spark DataFrame\n",
        "users = spark.read.csv('Users.csv', header=True, inferSchema=True)\n",
        "books = spark.read.csv('Books.csv', header = True, inferSchema=True)\n",
        "ratings = spark.read.csv('Ratings.csv', header=True, inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wflfS0K4pbN1",
        "outputId": "68022036-0013-4b80-d023-2975530e28eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- ISBN: string (nullable = true)\n",
            " |-- Book-Title: string (nullable = true)\n",
            " |-- Book-Author: string (nullable = true)\n",
            " |-- Year-Of-Publication: string (nullable = true)\n",
            " |-- Publisher: string (nullable = true)\n",
            " |-- Image-URL-S: string (nullable = true)\n",
            " |-- Image-URL-M: string (nullable = true)\n",
            " |-- Image-URL-L: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- User-ID: integer (nullable = true)\n",
            " |-- ISBN: string (nullable = true)\n",
            " |-- Book-Rating: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- User-ID: integer (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Show schemas\n",
        "books.printSchema()\n",
        "ratings.printSchema()\n",
        "users.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- User-ID: integer (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- ISBN: string (nullable = true)\n",
            " |-- Book-Title: string (nullable = true)\n",
            " |-- Book-Author: string (nullable = true)\n",
            " |-- Year-Of-Publication: integer (nullable = true)\n",
            " |-- Publisher: string (nullable = true)\n",
            " |-- Image-URL-S: string (nullable = true)\n",
            " |-- Image-URL-M: string (nullable = true)\n",
            " |-- Image-URL-L: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# prompt:  change the type of the Age from string to float\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "users = users.withColumn(\"Age\", users[\"Age\"].cast(IntegerType()))\n",
        "users.printSchema()\n",
        "\n",
        "books = books.withColumn(\"Year-Of-Publication\", books[\"Year-Of-Publication\"].cast(IntegerType()))\n",
        "books.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|summary|                ISBN|          Book-Title|         Book-Author|Year-Of-Publication|           Publisher|         Image-URL-S|         Image-URL-M|         Image-URL-L|\n",
            "+-------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|  count|              271360|              271360|              271359|             271303|              271358|              271360|              271360|              271357|\n",
            "|   mean|1.0412234356977516E9|            Infinity|              2001.0|  1959.754171535147|  3765.6153846153848|             1992.25|              1984.5|             1988.25|\n",
            "| stddev|1.4877142833957229E9|                 NaN|  1.7320508075688772| 258.01944980678707|  7963.0591536808415|   9.863845829363632|  0.7071067811865476|    7.13559154286923|\n",
            "|    min|          0000913154| A Light in the S...| 15th Illinois Vo...|                  0| As Taught at the...| 1865. (Collector...|          Bill Stern|       Our Miss b\"\"\"|\n",
            "|    max|          B0002K6K8O|   Ã?Â?thique en toc|      Ã?Â?ric Holder|               2050|      Ã?Â¶bv&amp;hpt|http://images.ama...|http://images.ama...|http://images.ama...|\n",
            "+-------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n",
            "+-------+-----------------+------------------+------------------+\n",
            "|summary|          User-ID|          Location|               Age|\n",
            "+-------+-----------------+------------------+------------------+\n",
            "|  count|           278858|            278859|            167674|\n",
            "|   mean|         139429.5|              NULL|34.751833915812824|\n",
            "| stddev|80499.51502027822|              NULL|14.431010268447515|\n",
            "|    min|                1|             milan|                 0|\n",
            "|    max|           278858|ýzmýr, n/a, turkey|               244|\n",
            "+-------+-----------------+------------------+------------------+\n",
            "\n",
            "+-------+------------------+-----------+------------------+\n",
            "|summary|           User-ID|       ISBN|       Book-Rating|\n",
            "+-------+------------------+-----------+------------------+\n",
            "|  count|           1149780|    1149780|           1149780|\n",
            "|   mean|140386.39512602412|   Infinity|2.8669501991685364|\n",
            "| stddev| 80562.27771851169|        NaN| 3.854183859201664|\n",
            "|    min|                 2| 0330299891|                 0|\n",
            "|    max|            278854|  Ô½crosoft|                10|\n",
            "+-------+------------------+-----------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "books.describe().show()\n",
        "users.describe().show()\n",
        "ratings.describe().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Detailed Age Statistics:\n",
            "Total records: 278859\n",
            "NULL ages: 111185\n",
            "Non-NULL ages: 167674\n",
            "Percentage of NULL ages: 39.87%\n",
            "\n",
            "Age Statistics (excluding NULLs):\n",
            "+-------+------------------+\n",
            "|summary|               Age|\n",
            "+-------+------------------+\n",
            "|  count|            167674|\n",
            "|   mean|34.751833915812824|\n",
            "| stddev|14.431010268447515|\n",
            "|    min|                 0|\n",
            "|    max|               244|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "import matplotlib.pyplot as plt\n",
        "# Create a figure with two subplots\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n",
        "\n",
        "# Plot 1: Age distribution (excluding NULLs)\n",
        "age_data = users.select(\"Age\").toPandas()\n",
        "age_data = age_data.dropna()\n",
        "age_data = age_data[age_data[\"Age\"] > 0]\n",
        "ax1.hist(age_data[\"Age\"], bins=30, edgecolor='black')\n",
        "ax1.set_title('Distribution of User Ages (Excluding NULLs)')\n",
        "ax1.set_xlabel('Age')\n",
        "ax1.set_ylabel('Count')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: NULL vs Non-NULL distribution\n",
        "null_count = users.filter(col(\"Age\").isNull()).count()\n",
        "non_null_count = users.filter(col(\"Age\").isNotNull()).count()\n",
        "ax2.bar(['NULL', 'Non-NULL'], [null_count, non_null_count])\n",
        "ax2.set_title('NULL vs Non-NULL Ages')\n",
        "ax2.set_ylabel('Count')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "total_count = users.count()\n",
        "# Print detailed statistics\n",
        "print(\"\\nDetailed Age Statistics:\")\n",
        "print(f\"Total records: {users.count()}\")\n",
        "print(f\"NULL ages: {null_count}\")\n",
        "print(f\"Non-NULL ages: {non_null_count}\")\n",
        "print(f\"Percentage of NULL ages: {(null_count/total_count)*100:.2f}%\")\n",
        "\n",
        "# Show age statistics for non-NULL values\n",
        "print(\"\\nAge Statistics (excluding NULLs):\")\n",
        "users.filter(col(\"Age\").isNotNull()).select(\"Age\").describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of users before filtering:\n",
            "278859\n",
            "Number of users after filtering:\n",
            "277205\n",
            "Percentage of users filtered out:\n",
            "0.59%\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Filter users between 10 and 80 years old\n",
        "print(\"Number of users before filtering:\")\n",
        "print(users.select(\"User-ID\").distinct().count())\n",
        "filtered_users = users.filter((users.Age.isNotNull() & (users.Age >= 10) & (users.Age <= 80)) | users.Age.isNull())\n",
        "\n",
        "print(\"Number of users after filtering:\")\n",
        "print(filtered_users.select(\"User-ID\").distinct().count())\n",
        "\n",
        "print(\"Percentage of users filtered out:\")\n",
        "print(f\"{(total_count - filtered_users.select('User-ID').distinct().count()) / total_count * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Count ratings\n",
        "zero_ratings = ratings.filter(ratings['Book-Rating'] == 0).count()\n",
        "nonzero_ratings = ratings.filter(ratings['Book-Rating'] > 0).count()\n",
        "\n",
        "# Create data for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rating_counts = [zero_ratings, nonzero_ratings]\n",
        "labels = ['Rating = 0', 'Rating 1-10']\n",
        "\n",
        "# Create bar plot\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(labels, rating_counts)\n",
        "plt.title('Distribution of Zero vs Non-Zero Ratings')\n",
        "plt.ylabel('Count')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Add value labels on top of each bar\n",
        "for i, v in enumerate(rating_counts):\n",
        "    plt.text(i, v, str(v), ha='center', va='bottom')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null values in users DataFrame:\n",
            "+-------+--------+------+\n",
            "|User-ID|Location|   Age|\n",
            "+-------+--------+------+\n",
            "|      1|       0|111185|\n",
            "+-------+--------+------+\n",
            "\n",
            "\n",
            "Null values in ratings DataFrame:\n",
            "+-------+----+-----------+\n",
            "|User-ID|ISBN|Book-Rating|\n",
            "+-------+----+-----------+\n",
            "|      0|   0|          0|\n",
            "+-------+----+-----------+\n",
            "\n",
            "\n",
            "Null values in books DataFrame:\n",
            "+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n",
            "|ISBN|Book-Title|Book-Author|Year-Of-Publication|Publisher|Image-URL-S|Image-URL-M|Image-URL-L|\n",
            "+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n",
            "|   0|         0|          1|                 57|        2|          0|          0|          3|\n",
            "+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check for null values in users DataFrame\n",
        "print(\"Null values in users DataFrame:\")\n",
        "users.select([count(when(col(c).isNull(), c)).alias(c) for c in users.columns]).show()\n",
        "\n",
        "# Check for null values in ratings DataFrame  \n",
        "print(\"\\nNull values in ratings DataFrame:\")\n",
        "ratings.select([count(when(col(c).isNull(), c)).alias(c) for c in ratings.columns]).show()\n",
        "\n",
        "# Check for null values in books DataFrame\n",
        "print(\"\\nNull values in books DataFrame:\") \n",
        "books.select([count(when(col(c).isNull(), c)).alias(c) for c in books.columns]).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After dropping nulls:\n",
            "\n",
            "Users count: 278858\n",
            "Ratings count: 1149780\n",
            "Books count: 271300\n"
          ]
        }
      ],
      "source": [
        "# Drop rows with null User-ID\n",
        "users = users.dropna(subset=['User-ID'])\n",
        "ratings = ratings.dropna(subset=['User-ID'])\n",
        "\n",
        "# Drop null values in books DataFrame\n",
        "books = books.dropna()\n",
        "\n",
        "print(\"After dropping nulls:\")\n",
        "print(\"\\nUsers count:\", users.count())\n",
        "print(\"Ratings count:\", ratings.count()) \n",
        "print(\"Books count:\", books.count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null values in users DataFrame after processing:\n",
            "+-------+--------+------+\n",
            "|User-ID|Location|   Age|\n",
            "+-------+--------+------+\n",
            "|      0|       0|111184|\n",
            "+-------+--------+------+\n",
            "\n",
            "\n",
            "Null values in ratings DataFrame after processing:\n",
            "+-------+----+-----------+\n",
            "|User-ID|ISBN|Book-Rating|\n",
            "+-------+----+-----------+\n",
            "|      0|   0|          0|\n",
            "+-------+----+-----------+\n",
            "\n",
            "\n",
            "Null values in books DataFrame after processing:\n",
            "+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n",
            "|ISBN|Book-Title|Book-Author|Year-Of-Publication|Publisher|Image-URL-S|Image-URL-M|Image-URL-L|\n",
            "+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n",
            "|   0|         0|          0|                  0|        0|          0|          0|          0|\n",
            "+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check for null values in users DataFrame after processing\n",
        "print(\"Null values in users DataFrame after processing:\")\n",
        "users.select([count(when(col(c).isNull(), c)).alias(c) for c in users.columns]).show()\n",
        "\n",
        "# Check for null values in ratings DataFrame after processing\n",
        "print(\"\\nNull values in ratings DataFrame after processing:\")\n",
        "ratings.select([count(when(col(c).isNull(), c)).alias(c) for c in ratings.columns]).show()\n",
        "\n",
        "# Check for null values in books DataFrame after processing\n",
        "print(\"\\nNull values in books DataFrame after processing:\")\n",
        "books.select([count(when(col(c).isNull(), c)).alias(c) for c in books.columns]).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of distinct users: 277205\n",
            "Number of distinct users in ratings: 105283\n"
          ]
        }
      ],
      "source": [
        "print('Number of distinct users:', filtered_users.select('User-ID').distinct().count())\n",
        "print('Number of distinct users in ratings:', ratings.select('User-ID').distinct().count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of distinct users in ratings: 104634\n"
          ]
        }
      ],
      "source": [
        "check_users = filtered_users.join(ratings, 'User-ID', 'inner')\n",
        "print('Number of distinct users in ratings:', check_users.select('User-ID').distinct().count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of distinct users in ratings: 91548\n"
          ]
        }
      ],
      "source": [
        "check_book = ratings.join(books, 'ISBN', 'inner')\n",
        "print('Number of distinct users in ratings:', check_book.select('User-ID').distinct().count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 9681:>                                                       (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of distinct users in ratings: 91548\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "check_total = check_users.join(check_book, 'User-ID', 'inner')\n",
        "print('Number of distinct users in ratings:', check_total.select('User-ID').distinct().count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ratings count before filtering: 1149780\n",
            "Ratings count after filtering: 1139222\n"
          ]
        }
      ],
      "source": [
        "# Keep only ratings from users that exist in filtered_users\n",
        "print(\"Ratings count before filtering:\", ratings.count())\n",
        "ratings = ratings.join(filtered_users.select(\"User-ID\"), \"User-ID\", \"inner\")\n",
        "\n",
        "print(\"Ratings count after filtering:\", ratings.count())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Xử lý users\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final feature set schema:\n",
            "root\n",
            " |-- User-ID: integer (nullable = true)\n",
            " |-- mean_pub_year: double (nullable = true)\n",
            " |-- median_pub_year: double (nullable = true)\n",
            " |-- std_dev_pub_year: double (nullable = true)\n",
            " |-- mean_author_encoded_age: double (nullable = true)\n",
            " |-- mean_publisher_encoded_age: double (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            "\n",
            "\n",
            "Sample of processed data:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 9634:========================>                            (11 + 10) / 24]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+---------------+------------------+-----------------------+--------------------------+----+\n",
            "|User-ID|     mean_pub_year|median_pub_year|  std_dev_pub_year|mean_author_encoded_age|mean_publisher_encoded_age| Age|\n",
            "+-------+------------------+---------------+------------------+-----------------------+--------------------------+----+\n",
            "|     53|1990.5714285714287|         1992.0| 6.477065767180455|      42.33489304812834|         37.53558997111198|NULL|\n",
            "|     78|            1988.0|         1988.0|              NULL|           32.572265625|         30.73865414710485|  18|\n",
            "|     85|            2001.0|         2001.0|1.4142135623730951|                33.2375|          35.0144213318442|  41|\n",
            "|    133|1991.6666666666667|         1991.0| 2.081665999466133|     41.549257105943155|         38.78149527887378|  45|\n",
            "|    243| 1997.506329113924|         1999.0| 5.806436344135757|      38.51132047009556|         37.74714772981151|NULL|\n",
            "+-------+------------------+---------------+------------------+-----------------------+--------------------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Step 1: Data Cleaning - Convert Year-Of-Publication to integer\n",
        "from pyspark.sql.functions import col, when, avg, stddev, mean, median\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Step 2: Join books with ratings to get user reading history\n",
        "user_book_history = ratings.join(books, \"ISBN\", 'inner')\n",
        "\n",
        "# Step 3: Calculate numerical features from Year-Of-Publication\n",
        "user_numerical_features = user_book_history.groupBy(\"User-ID\").agg(\n",
        "    mean(\"Year-Of-Publication\").alias(\"mean_pub_year\"),\n",
        "    median(\"Year-Of-Publication\").alias(\"median_pub_year\"),\n",
        "    stddev(\"Year-Of-Publication\").alias(\"std_dev_pub_year\")\n",
        ")\n",
        "\n",
        "# Step 4: Calculate target encoding for Book-Author\n",
        "# First, get average age per author from users with known ages\n",
        "author_age_mapping = user_book_history.join(filtered_users.filter(col(\"Age\").isNotNull()), \"User-ID\") \\\n",
        "    .groupBy(\"Book-Author\") \\\n",
        "    .agg(avg(\"Age\").alias(\"author_encoded_age\"))\n",
        "\n",
        "# Apply author encoding to all books\n",
        "books_with_author_encoding = books.join(author_age_mapping, \"Book-Author\", \"left\")\n",
        "\n",
        "# Calculate mean author encoding per user\n",
        "user_author_features = user_book_history.join(books_with_author_encoding, [\"ISBN\", \"Book-Author\"]) \\\n",
        "    .groupBy(\"User-ID\") \\\n",
        "    .agg(mean(\"author_encoded_age\").alias(\"mean_author_encoded_age\"))\n",
        "\n",
        "# Step 5: Calculate target encoding for Publisher\n",
        "publisher_age_mapping = user_book_history.join(filtered_users.filter(col(\"Age\").isNotNull()), \"User-ID\") \\\n",
        "    .groupBy(\"Publisher\") \\\n",
        "    .agg(avg(\"Age\").alias(\"publisher_encoded_age\"))\n",
        "\n",
        "# Apply publisher encoding to all books\n",
        "books_with_publisher_encoding = books.join(publisher_age_mapping, \"Publisher\", \"left\")\n",
        "\n",
        "# Calculate mean publisher encoding per user\n",
        "user_publisher_features = user_book_history.join(books_with_publisher_encoding, [\"ISBN\", \"Publisher\"]) \\\n",
        "    .groupBy(\"User-ID\") \\\n",
        "    .agg(mean(\"publisher_encoded_age\").alias(\"mean_publisher_encoded_age\"))\n",
        "\n",
        "# Step 6: Combine all features\n",
        "user_features = user_numerical_features \\\n",
        "    .join(user_author_features, \"User-ID\", \"left\") \\\n",
        "    .join(user_publisher_features, \"User-ID\", \"left\")\n",
        "\n",
        "# Step 7: Join with original users DataFrame to get target variable (Age)\n",
        "process_user_age = user_features.join(filtered_users.select(\"User-ID\", \"Age\"), \"User-ID\", \"left\")\n",
        "\n",
        "# Display the final feature set\n",
        "print(\"Final feature set schema:\")\n",
        "process_user_age.printSchema()\n",
        "\n",
        "print(\"\\nSample of processed data:\")\n",
        "process_user_age.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of distinct users: 91548\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of distinct users: {process_user_age.select('User-ID').distinct().count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null value counts per column:\n",
            "User-ID: 0 null values\n",
            "mean_pub_year: 0 null values\n",
            "median_pub_year: 0 null values\n",
            "std_dev_pub_year: 51003 null values\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean_author_encoded_age: 1551 null values\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean_publisher_encoded_age: 394 null values\n",
            "Age: 38261 null values\n",
            "\n",
            "Total rows in dataset: 91548\n",
            "\n",
            "Sample of rows containing null values:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 9879:=============>                                        (6 + 10) / 24]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+---------------+-----------------+-----------------------+--------------------------+----+\n",
            "|User-ID|     mean_pub_year|median_pub_year| std_dev_pub_year|mean_author_encoded_age|mean_publisher_encoded_age| Age|\n",
            "+-------+------------------+---------------+-----------------+-----------------------+--------------------------+----+\n",
            "|     53|1990.5714285714287|         1992.0|6.477065767180455|      42.33489304812834|         37.53558997111198|NULL|\n",
            "|     78|            1988.0|         1988.0|             NULL|           32.572265625|         30.73865414710485|  18|\n",
            "|    243| 1997.506329113924|         1999.0|5.806436344135757|      38.51132047009556|         37.74714772981151|NULL|\n",
            "|    463|            1995.0|         1995.0|             NULL|     25.023255813953487|                      24.0|  20|\n",
            "|    472|            1993.0|         1994.5|5.291502622129182|     36.395833333333336|         38.68284521048285|NULL|\n",
            "+-------+------------------+---------------+-----------------+-----------------------+--------------------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Check for null values in each column\n",
        "from functools import reduce\n",
        "print(\"Null value counts per column:\")\n",
        "for column in process_user_age.columns:\n",
        "    null_count = process_user_age.filter(col(column).isNull()).count()\n",
        "    print(f\"{column}: {null_count} null values\")\n",
        "\n",
        "# Check total null values in the dataset\n",
        "total_rows = process_user_age.count()\n",
        "print(f\"\\nTotal rows in dataset: {total_rows}\")\n",
        "\n",
        "# Show sample of rows with null values\n",
        "print(\"\\nSample of rows containing null values:\")\n",
        "process_user_age.filter(\n",
        "    reduce(lambda x, y: x | y, [col(c).isNull() for c in process_user_age.columns])\n",
        ").show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null value counts after filling:\n",
            "User-ID: 0 null values\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean_pub_year: 0 null values\n",
            "median_pub_year: 0 null values\n",
            "std_dev_pub_year: 0 null values\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean_author_encoded_age: 0 null values\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean_publisher_encoded_age: 0 null values\n",
            "Age: 38261 null values\n"
          ]
        }
      ],
      "source": [
        "# Fill null values with median for all columns except 'Age'\n",
        "from pyspark.sql.functions import col, when, lit\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# Get median values for each column\n",
        "median_values = {}\n",
        "for column in process_user_age.columns:\n",
        "    if column not in ['User-ID', 'Age']:  # Skip User-ID and Age columns\n",
        "        median = process_user_age.approxQuantile(column, [0.5], 0.01)[0]\n",
        "        median_values[column] = median\n",
        "\n",
        "# Fill null values with median for each column\n",
        "for column, median in median_values.items():\n",
        "    process_user_age = process_user_age.withColumn(\n",
        "        column,\n",
        "        when(col(column).isNull(), lit(median)).otherwise(col(column))\n",
        "    )\n",
        "\n",
        "# Verify null values after filling\n",
        "print(\"Null value counts after filling:\")\n",
        "for column in process_user_age.columns:\n",
        "    null_count = process_user_age.filter(col(column).isNull()).count()\n",
        "    print(f\"{column}: {null_count} null values\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 9551:===============================================>      (22 + 3) / 25]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+------------------+------------------+------------------+-----------------------+--------------------------+------------------+\n",
            "|summary|           User-ID|     mean_pub_year|   median_pub_year|  std_dev_pub_year|mean_author_encoded_age|mean_publisher_encoded_age|               Age|\n",
            "+-------+------------------+------------------+------------------+------------------+-----------------------+--------------------------+------------------+\n",
            "|  count|            104634|            104634|            104634|            104634|                 104634|                    104634|             61375|\n",
            "|   mean|139442.46466731656|1955.1148970907436|1964.2142276888965| 35.86704806505941|      36.83156003964847|         36.89414034152192|35.414680244399186|\n",
            "| stddev| 80572.10163891372|247.33460738749793|243.18239465203797|160.71176619655324|      4.423747347362032|         2.818338502241186|13.606569703385862|\n",
            "|    min|                 2|               0.0|               0.0|               0.0|                   10.0|                      13.0|                10|\n",
            "|    max|            278854|            2038.0|            2038.0|1417.0419894978413|                   80.0|                      80.0|                80|\n",
            "+-------+------------------+------------------+------------------+------------------+-----------------------+--------------------------+------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "process_user_age.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null values in mean_pub_year: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null values in median_pub_year: 0\n",
            "Null values in std_dev_pub_year: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null values in mean_author_encoded_age: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null values in mean_publisher_encoded_age: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+---------------+------------------+-----------------------+--------------------------+----+--------------------+------------------+\n",
            "|User-ID|     mean_pub_year|median_pub_year|  std_dev_pub_year|mean_author_encoded_age|mean_publisher_encoded_age| Age|            features|        prediction|\n",
            "+-------+------------------+---------------+------------------+-----------------------+--------------------------+----+--------------------+------------------+\n",
            "|     53|1990.5714285714287|         1992.0| 6.477065767180455|      42.33489304812834|         37.53558997111198|NULL|[1990.57142857142...|51.235266145861765|\n",
            "|    243| 1997.506329113924|         1999.0| 5.806436344135757|      38.51132047009556|         37.74714772981151|NULL|[1997.50632911392...|42.023636130445695|\n",
            "|    472|            1993.0|         1994.5| 5.291502622129182|     36.395833333333336|         38.68284521048285|NULL|[1993.0,1994.5,5....|31.839744738499046|\n",
            "|    513|            1989.0|         1989.0|4.6583258795408335|      33.86682808716707|         36.04533152909337|NULL|[1989.0,1989.0,4....| 29.24334329677949|\n",
            "|    516|            1995.0|         1995.0|4.6583258795408335|     38.723395270270274|         38.95392491467577|NULL|[1995.0,1995.0,4....|37.437879583292414|\n",
            "+-------+------------------+---------------+------------------+-----------------------+--------------------------+----+--------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 17979:=======================================>             (18 + 6) / 24]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root Mean Square Error on training data: 10.645629187640422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col, first\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Prepare features for Gradient Boosting\n",
        "feature_cols = [c for c in process_user_age.columns if c not in ['User-ID', 'Age']]\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_cols, \n",
        "    outputCol='features'\n",
        ")\n",
        "# Check for null values in feature columns\n",
        "for feature in feature_cols:\n",
        "    null_count = process_user_age.filter(col(feature).isNull()).count()\n",
        "    print(f\"Null values in {feature}: {null_count}\")\n",
        "# Split data into training and prediction sets\n",
        "train_data = process_user_age.filter(col('Age').isNotNull())\n",
        "predict_data = process_user_age.filter(col('Age').isNull())\n",
        "\n",
        "# Create and train Gradient Boosting model\n",
        "gbt = GBTRegressor(\n",
        "    featuresCol='features', \n",
        "    labelCol='Age', \n",
        "    maxIter=100, \n",
        "    maxDepth=5, \n",
        "    stepSize=0.1\n",
        ")\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline(stages=[assembler, gbt])\n",
        "\n",
        "# Train the model\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(predict_data)\n",
        "\n",
        "predictions.show(5)\n",
        "\n",
        "# Evaluate model performance on training data\n",
        "train_predictions = model.transform(train_data)\n",
        "evaluator = RegressionEvaluator(labelCol='Age', predictionCol='prediction')\n",
        "rmse = evaluator.evaluate(train_predictions)\n",
        "print(f\"Root Mean Square Error on training data: {rmse}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated user data with predicted ages:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 18935:>                                                      (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+\n",
            "|User-ID|Age|\n",
            "+-------+---+\n",
            "|     53| 51|\n",
            "|     78| 18|\n",
            "|     85| 41|\n",
            "|    133| 45|\n",
            "|    243| 42|\n",
            "+-------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.dataframe import DataFrame as SparkDataFrame\n",
        "\n",
        "filtered_users = filtered_users.join(ratings, 'User-ID', 'inner').select('User-ID', 'Age')\n",
        "\n",
        "# Convert filtered_users to PySpark DataFrame if it's a Pandas DataFrame\n",
        "if not isinstance(filtered_users, SparkDataFrame):\n",
        "    filtered_users = spark.createDataFrame(filtered_users)\n",
        "\n",
        "# Now proceed with the join operation\n",
        "predictions = predictions.withColumn(\"prediction\", col(\"prediction\").cast(\"integer\"))\n",
        "\n",
        "# Join predictions with filtered_users to update null ages\n",
        "updated_users = filtered_users.join(\n",
        "    predictions.select('User-ID', 'prediction'),\n",
        "    on='User-ID',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "updated_users = updated_users.withColumn(\n",
        "    'Age',\n",
        "    when(col('Age').isNull(), col('prediction'))\n",
        "    .otherwise(col('Age'))\n",
        ")\n",
        "\n",
        "# Drop the prediction column as it's no longer needed\n",
        "updated_users = updated_users.drop('prediction')\n",
        "updated_users = updated_users.select('User-ID', 'Age').distinct()\n",
        "# Drop rows where Age is null\n",
        "updated_users = updated_users.dropna(subset=['Age'])\n",
        "\n",
        "# Show the results\n",
        "print(\"Updated user data with predicted ages:\")\n",
        "updated_users.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.               (0 + 10) / 24]\n",
            "Traceback (most recent call last):\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of distinct users: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mupdated_users\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUser-ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1240\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/24 01:57:08 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 1032397 ms exceeds timeout 600000 ms\n",
            "25/05/24 01:57:08 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
            "25/05/24 01:57:25 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 01:57:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 01:58:25 WARN Executor: Issue communicating with driver in heartbeater]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 01:58:25 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 01:59:25 ERROR Inbox: Ignoring error                     (5 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 01:59:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:00:25 ERROR Inbox: Ignoring error                     (5 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:00:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:01:25 ERROR Inbox: Ignoring error                     (5 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/24 02:01:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/24 02:02:25 WARN Executor: Issue communicating with driver in heartbeater]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:02:25 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:03:25 ERROR Inbox: Ignoring error                     (6 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:03:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:04:25 ERROR Inbox: Ignoring error                     (6 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:04:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:05:25 ERROR Inbox: Ignoring error                     (6 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:05:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:06:25 ERROR Inbox: Ignoring error                     (6 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:06:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:07:25 ERROR Inbox: Ignoring error                     (6 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:07:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:08:25 WARN Executor: Issue communicating with driver in heartbeater]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:08:25 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:09:25 ERROR Inbox: Ignoring error                     (7 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:09:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:10:25 WARN Executor: Issue communicating with driver in heartbeater]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/24 02:10:25 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/24 02:11:25 WARN Executor: Issue communicating with driver in heartbeater]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:11:25 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:12:25 ERROR Inbox: Ignoring error                     (7 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:12:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:13:25 WARN Executor: Issue communicating with driver in heartbeater]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:13:25 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:14:25 ERROR Inbox: Ignoring error                     (8 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/24 02:14:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/24 02:15:25 WARN Executor: Issue communicating with driver in heartbeater]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:15:25 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:16:25 ERROR Inbox: Ignoring error                     (8 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:16:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:17:25 ERROR Inbox: Ignoring error                     (8 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:17:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:18:25 WARN Executor: Issue communicating with driver in heartbeater]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:18:25 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:19:25 ERROR Inbox: Ignoring error                     (8 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:19:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:20:25 ERROR Inbox: Ignoring error                     (8 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:20:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:21:25 ERROR Inbox: Ignoring error                     (9 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:21:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:22:25 ERROR Inbox: Ignoring error                     (9 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:22:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:23:25 WARN Executor: Issue communicating with driver in heartbeater]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:23:25 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:24:25 ERROR Inbox: Ignoring error                     (9 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:24:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:25:25 ERROR Inbox: Ignoring error                     (9 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:25:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:26:25 ERROR Inbox: Ignoring error                     (9 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:26:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:27:25 ERROR Inbox: Ignoring error                    (10 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:27:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:28:25 ERROR Inbox: Ignoring error                    (11 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:28:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:29:25 ERROR Inbox: Ignoring error                    (12 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:29:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:30:25 WARN Executor: Issue communicating with driver in heartbeater]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:30:25 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:31:25 ERROR Inbox: Ignoring error                    (12 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:31:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:32:25 ERROR Inbox: Ignoring error                    (13 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:32:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:33:25 WARN Executor: Issue communicating with driver in heartbeater]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/24 02:33:25 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/24 02:34:25 ERROR Inbox: Ignoring error                    (13 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:34:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:35:25 ERROR Inbox: Ignoring error                    (13 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/24 02:35:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/24 02:36:25 ERROR Inbox: Ignoring error                    (13 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:36:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:37:25 ERROR Inbox: Ignoring error                    (13 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/24 02:37:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/24 02:38:25 ERROR Inbox: Ignoring error                    (14 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:38:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:39:25 WARN Executor: Issue communicating with driver in heartbeater]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:39:25 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:40:25 ERROR Inbox: Ignoring error                    (14 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:40:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:41:25 ERROR Inbox: Ignoring error                    (14 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:41:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:42:25 ERROR Inbox: Ignoring error                    (14 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:42:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:43:25 WARN Executor: Issue communicating with driver in heartbeater]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:43:25 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:44:25 ERROR Inbox: Ignoring error                    (14 + 10) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 02:44:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 03:00:34 ERROR Inbox: Ignoring error=>                   (15 + 9) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 03:00:34 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 03:16:59 ERROR Inbox: Ignoring error===>                 (16 + 8) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 03:16:59 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 03:33:11 ERROR Inbox: Ignoring error=====>               (17 + 7) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 03:33:11 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 03:39:25 ERROR Inbox: Ignoring error=====>               (17 + 7) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 03:39:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 03:40:25 WARN Executor: Issue communicating with driver in heartbeater]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 03:40:25 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 03:41:25 ERROR Inbox: Ignoring error=======>             (18 + 6) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 03:41:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 03:42:25 ERROR Inbox: Ignoring error=======>             (18 + 6) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 03:42:25 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 04:01:10 ERROR Inbox: Ignoring error=======>             (18 + 6) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 04:01:10 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 04:17:54 ERROR Inbox: Ignoring error=======>             (18 + 6) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 04:17:54 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 04:18:54 ERROR Inbox: Ignoring error=======>             (18 + 6) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 04:18:54 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 04:19:54 ERROR Inbox: Ignoring error=======>             (18 + 6) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 04:19:54 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 04:20:54 ERROR Inbox: Ignoring error=======>             (18 + 6) / 24]\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 04:20:54 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:57856\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/24 04:20:54 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of distinct users: {updated_users.select('User-ID').distinct().count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'Age'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Age'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[93], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create the plot\u001b[39;00m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(\u001b[43mage_distribution_pd\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, age_distribution_pd[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistribution of User Ages\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Age'"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Convert Spark DataFrame to Pandas for plotting\n",
        "users_pd = updated_users.toPandas()\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=users_pd, x='Age', bins=30, kde=True)\n",
        "plt.title('Distribution of User Ages')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Count')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------------+----+\n",
            "|User-ID|            Location| Age|\n",
            "+-------+--------------------+----+\n",
            "|      1|  nyc, new york, usa|NULL|\n",
            "|      2|stockton, califor...|  18|\n",
            "|      3|moscow, yukon ter...|NULL|\n",
            "|      4|porto, v.n.gaia, ...|  17|\n",
            "|      5|farnborough, hant...|NULL|\n",
            "+-------+--------------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "users = users.drop(\"ISBN\")\n",
        "users.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of rows: 278858\n"
          ]
        }
      ],
      "source": [
        "print(f\"number of rows: {users.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Xử lý Rating\n",
        "Mình sẽ dùng implicit rating hệ (0,1)\n",
        "Đổi Nan, rating từ (1 ->5) là thành 0 (not good)\n",
        "Đổi 0, rating từ (5->10) là thành 1 (good)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOqCfJnIAAlu"
      },
      "source": [
        "## Xử lí user with null age "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD-eOLU7APxm"
      },
      "source": [
        "## xử lí tuổi bị null value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "collapsed": true,
        "id": "tL8_9f1Gp6pz",
        "outputId": "de927b53-8c56-45f7-f323-755effd13190"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2QAAAIjCAYAAABswtioAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAnvlJREFUeJzs3Qd4lFXaBuAnM5PeewgphJaE3qQpCoiiomtfCyprXXv9EXt3XQsoVnatuGLDVVdAEQREpfcaQgskQHrvbea/3jOZMYEASZjJN+W5r2uYduabk0xC5plzzns8TCaTCURERERERNTpdJ3/lERERERERCQYyIiIiIiIiDTCQEZERERERKQRBjIiIiIiIiKNMJARERERERFphIGMiIiIiIhIIwxkREREREREGmEgIyIiIiIi0ggDGRERERERkUYYyIiI3MgzzzwDDw+PTnmusWPHqpPFr7/+qp77m2++6ZTn/9vf/oZu3brBkVVUVOCWW25BTEyM+t7cf//9WneJiIg6GQMZEZGT+uSTT9SbeMvJx8cHsbGxmDhxIt58802Ul5fb5HmOHDmigtzmzZvhaBy5b23xj3/8Q72Od9xxB/7zn//g+uuvP25beY3vvvvuVu+TkCv3S+h1NA8//LDq21VXXaV1V4iIHJJB6w4QEdGpee6555CUlIT6+nrk5OSoN+Uy0jJjxgz88MMPGDBggLXtE088gUceeaTdoefZZ59Vo02DBg1q8+MWLVoEeztR395//30YjUY4sqVLl2LkyJF4+umn4YpMJhO++OIL9frMmzdPfUgQGBiodbeIiBwKR8iIiJzc+eefj+uuuw433ngjHn30Ufz888/45ZdfkJeXh7/85S+orq62tjUYDGokzZ6qqqrUuZeXlzppxdPTE97e3nBk8hqFhITAGVle5xORDwcOHTqEjz76CA0NDfj22287pW9ERM6EgYyIyAWNHz8eTz75JA4ePIjPPvvshGvIFi9ejDPOOEMFg4CAACQnJ+Oxxx6zvqE+7bTT1GUJfJbpkTLNTsgasX79+mHDhg0488wz4efnZ33s0WvILBobG1UbWTfl7++vQmNWVlaLNjKiImvAjtb8mCfrW2tryCorK/HQQw8hPj5ehTX5Wl977TU1ktPa9MDvv/9efX3Stm/fvli4cGGbg9bNN9+M6OhoFYAHDhyI2bNnH7OeLiMjAwsWLLD2/cCBA7CVPXv24PLLL1ffZ+lDXFwcrr76apSWlrZoJz8fQ4cOha+vL8LCwlSbo1+PE73OJzJnzhz06dMH48aNw4QJE9T11sjPqfwcyM9DVFQUHnjgAfXBQmvTMNesWYPzzjsPwcHBqh9nnXUWVqxY0aKNjMTJKLG8/vLayTHPOeccbNy4sR3fQSKizsEpi0RELkrWI8mbZpk6eOutt7baZseOHbjwwgvVtEaZ+ihvXvfu3Wt9g5uamqpuf+qpp3DbbbdhzJgx6vbRo0dbj1FYWKhG6eSNvIzUSQg5kRdffFG90Z42bZoKLm+88YZ6sy7rwCQUtFVb+tachC55079s2TIVlmSKo7zpnzp1Kg4fPozXX3+9Rfs//vhDjejceeedapqdrMuTgJOZmYnw8PDj9ktGJCXAyPdRQp1MJ507d64KiCUlJbjvvvtU32XNmAQPCUoSEkVkZCRsoa6uTq0lrK2txT333KNCmXyN8+fPV32QMGN5LSS4//Wvf1XFRfLz8/HWW2+p0LVp06YWo3ftfZ3luf/73/9av7ZrrrlGBWeZViv9aR6S5QOE7Oxs9b2R+z7//HP1OrU2xVP6IAFSpnnqdDp8/PHH6vG///47hg8frtrdfvvtal2dfP8lEErf5fVMS0vDkCFDbPI9JiKyGRMRETmljz/+WIZ1TOvWrTtum+DgYNPgwYOt159++mn1GIvXX39dXc/Pzz/uMeT40kae72hnnXWWum/WrFmt3icni2XLlqm2Xbt2NZWVlVlv//rrr9XtM2fOtN6WmJhomjJlykmPeaK+yePlOBbff/+9avvCCy+0aHfFFVeYPDw8THv37rXeJu28vLxa3LZlyxZ1+1tvvWU6kTfeeEO1++yzz6y31dXVmUaNGmUKCAho8bVL/yZNmnTC4zXv01133dXqfXPnzlX3y/dYbNq0SV2X24/nwIEDJr1eb3rxxRdb3L5t2zaTwWBocfuJXufj+eabb9Rj9uzZo67L1+3j46N+5pqbPn26aievj0V1dbUpJSWlxddkNBpNvXr1Mk2cOFFdtqiqqjIlJSWZzjnnnBY/98f7XhERORpOWSQicmEyBfFE1RYtIyD/+9//OlwAQ0bVZOSjrW644YYWhR2uuOIKdOnSBT/++CPsSY6v1+tx7733trhdRnAk7/z0008tbpdRux49elivyyhiUFAQ9u/ff9LnkVEeGRFqvp5NnlfK3C9fvhz2ZhkBkxHA4631ktE/ec1ldKygoMB6kr736tXrmBGq9r7OMj1x2LBh6Nmzp7our/mkSZOOmbYo00C7du2qRi8tZIrl0aO6MoIq0zCvvfZaNeJl6a+MsJ199tn47bffrD/D8nMtUxul6AsRkaNjICMicmESAE5U1U5KkZ9++ulquppMQZPpaF9//XW7wpm8mW5P8Q55s9+cTF+UN+22XD91vHVKsi3A0d8PmT5oub+5hISEY44RGhqK4uLikz6PfI0yna4tz2NLlvWBMk3ywQcfxAcffICIiAg1ffGdd95psX5Mwo0EUemrTJVsfpKpfTKdtKOvs0yLlGAq67tk6qblJD9r69evx+7du61t5fshwffotY2WINe8v2LKlCnH9Fe+Tpkiafn6XnnlFWzfvl2tFZRpjLJ28mRBmohIK1xDRkTkoqS6nbxBPfqNbXOyZktGFmQ0RIpLyGjFV199pdbkyNozGVE6mfas+2qr421eLQVB2tInWzje8xxdAKSzyAhV84qZzVlGwZpX0Jw+fbpatyajn/JaygjdSy+9hNWrV6t1axK65fssI4Otfa0yutrR11nWzElAkj7I6WgySibbFbSH5UOCV1999bjbL1j6LKN+sqbwu+++U1+7PObll19Wo4KyBo2IyJEwkBERuSgpGiFkdOREZCRHpnzJSfYuk82KH3/8cRXSZNre8cJRR1lGOpoHHBk9ab5fmoxEySjL0WQ0pXv37tbr7elbYmKi2g7g6L2wdu3aZb3fFuQ4W7duVQGi+SjZqT6PPC49Pb3V+yy3H33s/v37q5PsP7dy5Uo1QjVr1iy88MILalRKvvcymta7d2/YkgQuqcrY2v5q//rXv1TRDksgkz7v3LlT9aX56yk/E81Zpo/KtFH5uTwZmQYrBVnkJKN9UsxDipgwkBGRo+GURSIiFyTV6J5//nn1Znvy5MnHbVdUVHTMbZbRBxnhEFKKXLQWkDri008/bbGuTarhSYW95m+U5c23jORItUALqRB4dDn29vTtggsuUCNsb7/9dovbpbqiBAFbvVGX55FKgjLSaCF7cEn1QhnBkWl8HT2ufE+k9Hxz8rVLAJLXzVK9sKysTD1ncxLMJCBaXtfLLrtMjYxJMDp61E+uyzqtjpDXSEZdZZRK1gcefZJ1aBK2ZI2X5QMDqQApm5hb1NTUqI29m5PKivJzIdsUyFTco0mFSCGv8dGl/aXsvUxXtXztRESOhCNkREROTqacyeiLvAHPzc1VYUz2FpORB3mTe6KNoKVsvLx5lmIL0l5GEt599101pU32JhPyJliKJMjIiowsSQgaMWKECnsdIXtdybHljbn0V8rey7TK5kUcZE2bBDXZb0re2O/bt0/tl9W8yEZ7+3bRRRep/bBk9E/Wq8neYDKdTab0yZ5VRx+7o6QEv4wCyXRBCU+yF5Z8LbKVgHytJ1rTdyKPPPKImgooJen//ve/IyUlRRWtkH3XJNBK+XcL+RmQku9XXnmlGv2Snw0ZMZUAJqX7hXy9MlImm4nL9+OSSy5RfZO90WSqn3wd//d//9fufsrol2WLgeMFS9mgXEKkvFbytUhIliIoUvZeRrbkPsvPrWXUTMKkrBWT4Cx7wsnPj6xrkzAno7kycjZv3jwV9uXnV8KfvMYSgmVkdN26da1OnyQi0pzWZR6JiOjUyt5bTlKmPSYmRpX/lhLyzcurH6/s/ZIlS0wXX3yxKTY2Vj1ezq+55hrT7t27Wzzuf//7n6lPnz6qHHrzMvNSDr1v376t9u94Ze+/+OIL06OPPmqKiooy+fr6qrLvBw8ePObxUg5dSuR7e3ubTj/9dNP69euPOeaJ+nZ02XtRXl5ueuCBB9TX6enpqcqov/rqqy3KqJ+oxPzxyvEfLTc313TjjTeaIiIi1Pe1f//+rZbmb0/Ze3Ho0CHTLbfcor4v8vWGhYWZLrzwQtPq1atbtNu/f7/ppptuMvXo0UOVmpd248aNM/3yyy/HHPO///2v6YwzzjD5+/urk5Sbl689PT3d2uZEr/PR5GtNSEg4YZuxY8eq17++vt7aX/k+yM9DZGSk6aGHHlL9ktfh6K9NSvpfdtllpvDwcPWzId/Dv/71r+pnWdTW1pqmTp1qGjhwoCkwMFB9TXL53XffbVP/iYg6m4f8o3UoJCIiImpORhNl42wpTiMjYUREroqBjIiIiDQl1SObV3GUNWSDBw9W68Gal8gnInJFXENGREREmpICI7LvmxQmkYIcsl5Q1kUevYk0EZErYiAjIiIiTUmlRSnYIQFMRsX69OmDL7/8Um1cTkTk6jhlkYiIiIiISCPch4yIiIiIiEgjDGREREREREQa4RoyGzEajWqDTtlU07KJJRERERERuR+TyaQ2qo+NjVUb258IA5mNSBiLj4/XuhtEREREROQgsrKyEBcXd8I2DGQ2IiNjlm96UFCQ1t0hIiIiIiKNlJWVqcEaS0Y4EQYyG7FMU5QwxkBGREREREQebVjKxKIeREREREREGmEgIyIiIiIi0ggDGRERERERkUYYyIiIiIiIiDTCQEZERERERKQRBjIiIiIiIiKNMJARERERERFphIGMiIiIiIhIIwxkREREREREGmEgIyIiIiIi0ggDGRERERERkUYYyIiIiIiIiDTCQEZERERERKQRBjIiIiIiIiKNMJARERERERFphIGMiIiIiIhIIwxkREREREREGmEgIyIiIiIi0ohBqycmInIWmZmZKCgosMuxIyIikJCQYJdjExERkeNjICMiOkkYS0lNRXVVlV2O7+vnh11paQxlREREboqBjIjoBGRkTMLY5GmvIjqhh02PnZu5D3Nenqqeg4GMiIjIPTGQERG1gYSxuF59te4GERERuRgW9SAiIiIiItIIAxkREREREZFGGMiIiIiIiIg0wkBGRERERESkEQYyIiIiIiIijTCQERERERERaYSBjIiIiIiISCMMZERERERERBphICMiIiIiItIIAxkREREREZFGGMiIiIiIiIg0wkBGRERERESkEQYyIiIiIiIijTCQERERERERaYSBjIiIiIiISCMMZERERERERBphICMiIiIiItIIAxkREREREZFGGMiIiIiIiIg0wkBGRERERETkjoGssbERTz75JJKSkuDr64sePXrg+eefh8lksraRy0899RS6dOmi2kyYMAF79uxpcZyioiJMnjwZQUFBCAkJwc0334yKiooWbbZu3YoxY8bAx8cH8fHxeOWVV47pz9y5c5GSkqLa9O/fHz/++KMdv3oiIiIiInJ3mgayl19+Ge+99x7efvttpKWlqesSlN566y1rG7n+5ptvYtasWVizZg38/f0xceJE1NTUWNtIGNuxYwcWL16M+fPn47fffsNtt91mvb+srAznnnsuEhMTsWHDBrz66qt45pln8O9//9vaZuXKlbjmmmtUmNu0aRMuueQSddq+fXsnfkeIiIiIiMideJiaD0d1sgsvvBDR0dH48MMPrbddfvnlaiTss88+U6NjsbGxeOihh/B///d/6v7S0lL1mE8++QRXX321CnJ9+vTBunXrMGzYMNVm4cKFuOCCC3Do0CH1eAl9jz/+OHJycuDl5aXaPPLII/j++++xa9cudf2qq65CZWWlCnQWI0eOxKBBg1QYPBkJfcHBwap/MlJHRK5h48aNGDp0KB5851vE9epr02Mf2rMDM+66TH1QNGTIEJsem4iIiLTTnmyg6QjZ6NGjsWTJEuzevVtd37JlC/744w+cf/756npGRoYKUTJN0UK+sBEjRmDVqlXqupzLNEVLGBPSXqfTqRE1S5szzzzTGsaEjLKlp6ejuLjY2qb581jaWJ7naLW1teob3fxERERERETUHgZoSEapJMjIui29Xq/WlL344otqCqKQMCZkRKw5uW65T86joqJa3G8wGBAWFtaijaxTO/oYlvtCQ0PV+Yme52gvvfQSnn322VP8DhARERERkTvTdITs66+/xpw5c/D555+raUGzZ8/Ga6+9ps4d3aOPPqqGIC2nrKwsrbtERERERERORtMRsqlTp6pRMlkLJqSy4cGDB9Xo05QpUxATE6Nuz83NVVUWLeS6rO0S0iYvL6/FcRsaGlTlRcvj5Vwe05zl+snaWO4/mre3tzoRERERERE55QhZVVWVWuvVnExdNBqN6rJMM5RAJOvMLGSKo6wNGzVqlLou5yUlJWpRvMXSpUvVMWStmaWNVF6sr6+3tpGKjMnJyWq6oqVN8+extLE8DxERERERkUsFsosuukitGVuwYAEOHDiA7777DjNmzMCll16q7vfw8MD999+PF154AT/88AO2bduGG264QVVOlJL0IjU1Feeddx5uvfVWrF27FitWrMDdd9+tRt2knbj22mtVQQ8paS/l8b/66ivMnDkTDz74oLUv9913n6rOOH36dFV5Ucrir1+/Xh2LiIiIiIjI5aYsyn5jsjH0nXfeqaYdSoD6+9//rjaCtnj44YdVOXrZV0xGws444wwVnGTzZgtZhybB6eyzz1YjblI6X/Yua16ZcdGiRbjrrrtU+eqIiAj1HM33KpOKj7KW7YknnsBjjz2GXr16qbL4/fr168TvCBERERERuRNN9yFzJdyHjMg1cR8yIiIictl9yIiIiIiIiNwZAxkREREREZFGGMiIiIiIiIg0wkBGRERERESkEQYyIiIiIiIijTCQERERERERaYSBjIiIiIiISCMMZERERERERBphICMiIiIiItIIAxkREREREZFGGMiIiIiIiIg0wkBGRERERESkEQYyIiIiIiIijTCQERERERERaYSBjIiIiIiISCMMZERERERERBphICMiIiIiItIIAxkREREREZFGGMiIiIiIiIg0wkBGRERERESkEQYyIiIiIiIijTCQERERERERaYSBjIiIiIiISCMMZERERERERBphICMiIiIiItIIAxkREREREZFGGMiIiIiIiIg0wkBGRERERESkEQYyIiIiIiIijTCQERERERERaYSBjIiIiIiISCMMZERERERERBphICMiIiIiItIIAxkREREREZFGGMiIiIiIiIg0wkBGRERERESkEQYyIiIiIiIijTCQERERERERaYSBjIiIiIiISCMMZERERERERBphICMiIiIiInLHQNatWzd4eHgcc7rrrrvU/TU1NepyeHg4AgICcPnllyM3N7fFMTIzMzFp0iT4+fkhKioKU6dORUNDQ4s2v/76K4YMGQJvb2/07NkTn3zyyTF9eeedd1R/fHx8MGLECKxdu9bOXz0REREREbk7TQPZunXrkJ2dbT0tXrxY3X7llVeq8wceeADz5s3D3LlzsXz5chw5cgSXXXaZ9fGNjY0qjNXV1WHlypWYPXu2CltPPfWUtU1GRoZqM27cOGzevBn3338/brnlFvz888/WNl999RUefPBBPP3009i4cSMGDhyIiRMnIi8vr1O/H0RERERE5F40DWSRkZGIiYmxnubPn48ePXrgrLPOQmlpKT788EPMmDED48ePx9ChQ/Hxxx+r4LV69Wr1+EWLFmHnzp347LPPMGjQIJx//vl4/vnn1WiXhDQxa9YsJCUlYfr06UhNTcXdd9+NK664Aq+//rq1H/Ict956K2688Ub06dNHPUZG3D766CPNvjdEREREROT6HGYNmQQoCVY33XSTmra4YcMG1NfXY8KECdY2KSkpSEhIwKpVq9R1Oe/fvz+io6OtbWRkq6ysDDt27LC2aX4MSxvLMeR55bmat9HpdOq6pU1ramtr1fM0PxERERERETllIPv+++9RUlKCv/3tb+p6Tk4OvLy8EBIS0qKdhC+5z9KmeRiz3G+570RtJEBVV1ejoKBATX1srY3lGK156aWXEBwcbD3Fx8ef0tdPRERERETux2ECmUxPlCmHsbGxcAaPPvqomlZpOWVlZWndJSIiIiIicjIGOICDBw/il19+wbfffmu9TdaUyXRCGTVrPkomVRblPkubo6shWqowNm9zdGVGuR4UFARfX1/o9Xp1aq2N5RitkYqNciIiIiIiInLqETIp1iEl66UaooUU8fD09MSSJUust6Wnp6sy96NGjVLX5Xzbtm0tqiFKpUYJW1Kcw9Km+TEsbSzHkGmR8lzN2xiNRnXd0oaIiIiIiMglR8gk/EggmzJlCgyGP7sj67JuvvlmVY4+LCxMhax77rlHhaSRI0eqNueee64KXtdffz1eeeUVtebriSeeUHuXWUavbr/9drz99tt4+OGHVcGQpUuX4uuvv8aCBQuszyXPIc8/bNgwDB8+HG+88QYqKytV1UUiIiIiIiKXDWQyVVFGvSQsHU1K00vFQ9kQWqoaSnXEd99913q/TDWUUvl33HGHCmr+/v4qWD333HPWNlLyXsKX7Gk2c+ZMxMXF4YMPPlDHsrjqqquQn5+v9i+TUCcl9BcuXHhMoQ8iIiIiIiJb8jCZTCabHtFNSdVGGdWTAh8ymkdErkE2i5dpzQ++8y3ievW16bEP7dmBGXddprbeGDJkiE2PTURERM6RDRxiDRkRkavLKa3Bj9uykVdWo3VXiIiIyIEwkBER2VlJVR3+t+Uw9uRVYN7WbNQ2NGrdJSIiInIQDGRERHYk4WvelmzU1BvV9YraBvyanq91t4iIiMhBMJAREdmJ0WjCT9tyUFRVhwBvAy7oHwMPALtyyrEnt1zr7hEREZED0LzKIhGRrazeX4j7vtyEcH9vpHQJRJ8uQRibHImeUYGa9Of3vQU4WFQFg84DFw3ogqggH+R3q8W6A8VYuisP41nIlYiIyO1xhIyIXIIUjH1hwU7kltViZ3YZvt14GC8sSMMFb/6BXTllnd6f4so6bM4qUZfP7RutwpgYkRSOyEBv1DQYsaGQn4kRERG5OwYyInIJsi5r++Ey+Hnp8dY1g/HAhN5I7RKEugYjXvs5vdP7I6FQdAv3Q69mI3R6nQcm9omGzgPIrdHBENa10/tGREREjoOBjIhcYnTszaV71OXrRibiooGxuG9CL7x97WAVgH5Jy8P6A0Wd1h+jyaTWiQmZNnm08ABvdAn2VZd94my7txkRERE5FwYyInJ6K/YWYlNmCbwNOtwyJsl6e4/IAPx1WJy6/PLCXSq4dYZDxdWqmqL0JynCv9U2XUPMgcw7vl+n9ImIiIgcEwMZETm9N5eYR8euGZ6AqEDzWi2Le8/upYKRFNLorHLzlumKvaMDYdC3/t9sbIi5nz5xfTqlT0REROSYGMiIyOkrK649UAQvvQ63n9XjmPtlauDfRnezjpJJKXp77zu2L6/iuNMVm/fLAyYYQmJQUMWNoomIiNwVAxkRObW3l+5V51cOi0NMcMvRMYs7xvZAoI9Brev6YcsRu/Znb14FGowmhPp5IjrI+7jtvAw6hHiZw+HO/Dq79omIiIgcFwMZETmt3LIa/LG3AB4eaHV0zCLEz8t6/6zl++y6liwt21zMQyo8ekjHTiDCm4GMiIjI3TGQEZHTWrG3QJ337xqM+DC/E7a9bkSiWksmo2QbM837g9laaXU9DpdUq8spMSffjDrc26jO0woYyIiIiNwVAxkROS0ZHROn94w4adtgP09VDl/MWXPQLv1JayrmkRDmh0Afz5O2t4yQZZU1oLCi1i59IiIiIsfGQEZETkmmHa7cW6gun97j5IFMTB6RoM7nb81GSVWdzftjCWSpXU4+Oia89UBdgTkcShVIIiIicj8MZETklPblVyKnrEYVxxjWLbRNjxkUH4K+sUGoazDimw2HbNqfIyU1KKtpUNUeZf+ztqrN2qHO13XixtVERETkOBjIiMip148NSwyFj6e+TY+RIhuTRySqy3PWZNq0uIdl77Fe0QHwPM7eY62paQpkazMYyIiIiNwRAxkROXUga8v6seb+MigWAd4GZBRUYtU+85THU1XfaMSevKbqijHH33usNbWHzIFsx5FSVNQ22KQ/RERE5DwYyIjI6TQ0GrFqf2GHApmEsUsGW4p7ZNqkP7IRdH2jCcG+nogNaX0vtONpLC9AlL8esl/1hoNcR0ZERORuGMiIyOlsP1KG8poGtdmzlLxvr2uHm6ct/rwjR+1ldqp25jQV84gJPOneY63pE+Glztdm2GbEjoiIiJwHAxkROe10xVHdw6HXdSAAxQaptWcNRhM+/CPjlPpSXlOPrKJq62bQHZEcbi6Rv/2wOdgRERGR+2AgIyKnDWRn9GrfdMXm7hzXQ51/tvogiis7XgI/Lce8dqxriC+CfE++91hrEkLMj9udaz4WERERuQ8GMiJyKtV1jVjftGfX6DbuP9aacclR6NMlCFV1jfh4RUan7T3WmoQggzrPLq1BaXV9h49DREREzoeBjIicyvqDRahrNCImyAc9Iv07fBxZ63X3+J7q8icrD6Cspv1B6EBhFUqq6mHQeaBXVMcDmb+XDl2CzcVA9jZVayQiIiL3wEBGRE5lZVOp+tE9wztUQKO58/rGoGdUgNrQ+T+rDrbrsUaTCSv2madODogLVhtUn4re0eZAl55TcUrHISIiIufCQEZETmVD03TFEUlhp3wsnc4Dd441ryWT4h5VdW3fByw9pxyFFXXwNuhwWrdT70tyjDmQcR0ZERGRe2EgIyKnIRswbzlUoi4PTQy1yTH/MjAW8WG+KKqsw+dt3Jes+T5ow7qFwsdTf8r9+HOEjIGMiIjInTCQEZHT2HmkDLUNRrUBc/eIAJsc06DX4c6x5rVk0xftthbpOJGth0rVPmiyyfSguBCb9CO5KZBxhIyIiMi9MJARkdPYcNA8XXFIQoiabmgrVw6Nw5heEaiub8Stn65Xo2XHU1vfiLUHitTlkd3DVKCzBVnLJkviCivrUFBRa5NjEhERkeNjICMip7Ex0xLIbDNd0UJC1VvXDEZiuB8OFVfjzjkb1PTI1qYqLk3PU6N04f5eHd4IujW+Xnokhvmpy7s5bZGIiMhtMJARkdPY2DRCZqv1Y82F+Hnh/RuGwd9Lj9X7i/DsvB2oa/gzlJXXA1+tz8LuXHMVRBlR051ilcfjriPjtEUiIiK3wUBGRE4hu7QaR0prIDMVB8bbZt1Wa4HojasHq8ufrc7EoOcW4cXfixAy5joszfFEQUUdfD31uGRQLBLDO74H2vGw0iIREZH7YSAjIqew8aC5umJKTBD8vQ12e55z+kTjxUv7ISLAC1V1jdiQXYvg0VejweSBriG+uHZEgl3CmGClRSIiIvfDQEZETlXQwx7TFY82eUQi1j42AT/eOwY3DAhE1e5V6BfcgMsGd1WVFe3lzxGyCphMJrs9DxERETkOBjIicq6CHon2ma54NKni2Cc2CJekBCD/uxeRHGy0aWXH1nQL94dB54GK2gY1PZOIiIhcHwMZETm8mvpG7DhSqi4PTQiDq/Iy6NA90jwdkpUWiYiI3AMDGRE5vO2HS1HfaEJEgDfiw3zhylhpkYiIyL0wkBGRU20I7WHjUvOOJrkpkHGEjIiIyD0wkBGRw+vMgh5a691U2IMjZERERO7BfuXC2ujw4cOYNm0afvrpJ1RVVaFnz574+OOPMWzYMHW/VBp7+umn8f7776OkpASnn3463nvvPfTq1ct6jKKiItxzzz2YN28edDodLr/8csycORMBAQHWNlu3bsVdd92FdevWITIyUrV/+OGHW/Rl7ty5ePLJJ3HgwAF1/JdffhkXXHBBJ343iLSTmZmJgoICux0/IiICCQkJ7X6c/B+wMdNc8n6IGwQyywjZnrwKNBpN0Nu5kAgRERG5cSArLi5WAWvcuHEqkElQ2rNnD0JD/3zT9corr+DNN9/E7NmzkZSUpALTxIkTsXPnTvj4+Kg2kydPRnZ2NhYvXoz6+nrceOONuO222/D555+r+8vKynDuuediwoQJmDVrFrZt24abbroJISEhqp1YuXIlrrnmGrz00ku48MIL1WMvueQSbNy4Ef369dPoO0TUeWEsJTUV1VVVdnsOXz8/7EpLa3coyyyqQkFFLTz1HujfNRiuLj7MDz6eOtTUG3GwsBLdI//8YImIiIhcj6aBTEag4uPj1YiYhYSu5p+Mv/HGG3jiiSdw8cUXq9s+/fRTREdH4/vvv8fVV1+NtLQ0LFy4UI18WUbV3nrrLTWy9dprryE2NhZz5sxBXV0dPvroI3h5eaFv377YvHkzZsyYYQ1kMqJ23nnnYerUqer6888/rwLe22+/rUIckSuTkTEJY5OnvYrohB42P35u5j7MeXmqep72BrI1GUXqfEBcCHw89XB1MiLWKyoQ2w6Xqv3IGMiIiIhcm6aB7IcfflCjXVdeeSWWL1+Orl274s4778Stt96q7s/IyEBOTo4a2bIIDg7GiBEjsGrVKhXI5FxGuixhTEh7mbq4Zs0aXHrpparNmWeeqcKYhTyvBEIZpZMROWnz4IMPtuiftJHg15ra2lp1spBROCJnJ2EsrldfOJK1TYFseJLrlrtvrdKiOZCV47x+MVp3h4iIiFy1qMf+/fut68F+/vln3HHHHbj33nvV9EQhYUzIiFhzct1yn5xHRUW1uN9gMCAsLKxFm9aO0fw5jtfGcv/RZGqjhEPLSUb6iMj21h1wv0CWHGMeFWNhDyIiItenaSAzGo0YMmQI/vGPf2Dw4MFq+qCMjjnDFMFHH30UpaWl1lNWVpbWXSJyOTmlNThYWAWpa+EOFRaP3ouMpe+JiIhcn6aBrEuXLujTp0+L21JTU1WBARETY56qk5ub26KNXLfcJ+d5eXkt7m9oaFCVF5u3ae0YzZ/jeG0s9x/N29sbQUFBLU5EZFtrm0bH+sQGIcjHE+4iuan0fUZBJWobGrXuDhEREblqIJMKi+np6S1u2717NxITE60FPiQQLVmypMVaLVkbNmrUKHVdzqUc/oYNG6xtli5dqkbfZK2Zpc1vv/2mKjBaSMGO5ORka0VHadP8eSxtLM9DRJ1vbUahOh/eLRzuJCbIB4E+BjQYTSqUERERkevSNJA98MADWL16tZqyuHfvXlVq/t///rfaL0x4eHjg/vvvxwsvvKAKgEi5+htuuEFVTpSS9JYRNamOKFMd165dixUrVuDuu+9WBT+knbj22mtVQY+bb74ZO3bswFdffaWqKjYv4nHfffepao3Tp0/Hrl278Mwzz2D9+vXqWESkDXcs6GH5v8+yH1k6py0SERG5NE0D2WmnnYbvvvsOX3zxhdrrS0rNS5l72VfMQjZvlk2cZX2ZtK+oqFDBybIHmZCy9ikpKTj77LNVufszzjhDBTsLKbqxaNEiVbVx6NCheOihh/DUU09ZS96L0aNHWwPhwIED8c0336gKi9yDjEgbRZV1quy7OK2b+6wfs+jdNG1RKi0SERGR69K07L2QTZjldKJPip977jl1Oh6pqGjZBPp4BgwYgN9///2EbaT8vpyIyHGqK/aKCkB4gPcJ28q6U9njzB5kr0Mt/DlCZg6lRERE5Jo0D2RERKcyXVHCWEpqqtrY2p5kdF6TSoscISMiInJpDGRE5NT7j8nImISxydNeVRtb21ra2uX4afZM1NTUoDP1jjbvRZZZVIWqugb4efG/ayIiIlfEv/BE5HAqahuw/XBpuwp6SBiL69XX5n3JzdwHLcg0zYgAbxRU1GJPbgUGxodo0g8iIiJy4aIeRESt2XCwGEYTEB/miy7BvnBXllGydE5bJCIiclkMZETkcNx1/7HjriNj6XsiIiKXxUBGRA7nt93miokju7vX/mNHS24qfc8RMiIiItfFQEZEDiW3rAbbDpfCwwMYmxwFd8ZKi0RERK6PgYyIHMrSXXnqfGBcCCIDT7z/mLusIcstq0VpVb3W3SEiIiI7YCAjIoeyJC1XnU9Ide/RMRHo44muIeaiJrvzOEpGRETkihjIiMhh1NQ34o+95vVj41Oite6OY1VaZGEPIiIil8RARkQOY+W+AtTUGxEb7IPULub1U+6ud1NhD64jIyIick0MZETkMJakmdePjU+NgodU9SCkNAUyKXRCREREroeBjIgcgslkshb0ODuV0xUthiaYS/9vP1yqpnQSERGRa2EgIyKHsDO7DNmlNfD11GNUd/feELq5+DBfRAV6o77RhC1ZJVp3h4iIiGyMgYyIHGq64uk9I+Djqde6Ow5Dpm6e1s08Srb+YLHW3SEiIiIbYyAjIoewpGm6IsvdH2tYt1B1vu5AUYeP8eXaTJz3xm/q3Gg02bB3RERE1KmBbPbs2ViwYIH1+sMPP4yQkBCMHj0aBw8ePKXOEJF7yiursU7HG5/CQHY0ywjZhoPFaOxAmFqbUYTHv9+OXTnleOTbbbj6/dXYm1dhh54SERGR3QPZP/7xD/j6mjcqXbVqFd555x288soriIiIwAMPPNDuDhARfbPxkDofkhCCqCAfrbvjkJUWA7wNKK9paPd+ZAUVtbjni40qyA2KD1Fr9CSgXTDzd8xeecBufSYiIiI7BbKsrCz07NlTXf7+++9x+eWX47bbbsNLL72E33//vb2HIyI3J9PnvlybpS5fPTxB6+44JINeh8EJIery+oNtn7YoIez+Lzcjt6wWPaMCMOeWEVj0wJk4q3ck6hqNeHbeDmQWVtmx50RERGTzQBYQEIDCwkJ1edGiRTjnnHPUZR8fH1RXV7f3cETk5lbsK0BmURUCfQy4aECs1t1x+GmL6w60vbDHm0v24I+9BWpU7L3JQ+DvbUB8mB8+ufE0nNk7EjL78f3f99ux10RERGTzQCYB7JZbblGn3bt344ILLlC379ixA926dWvv4YjIzX2xNlOdXzq4K3y9WF3xpIU9MorUnm0nk5ZdhjeX7lGXX7y0H3pFmzeYtlRuvOOsHury1+uz1LRGIiIicpJAJmvGpIBHfn4+/vvf/yI83Lxf0IYNG3DNNdfYo49E5KLyy2uxaEeuunz1aZyueCKy/sug80BOWQ0Ol5x8NsKs5fsgue38fjG4bEjcMfeP7B6GgfEhqG0w4pMVXEtGRESkFUN7Gjc0NODNN9/EtGnTEBfX8g/8s88+a+u+EZGLm7shCw1NxSb6xAZp3R2H5udlQN+uwaoa5foDxYgL9Ttu26yiKszfmq0u3zXOvOb3aOZRsu64/bON+HTVAdw+tocqHEJERESdq11/fQ0Gg6qoeMMNN9ivR0TkdsU8rh3B0bG2OC0xVAUy2Y/sksFdj9vuwz8yVEGPM3pGoF/X4OO2O6dPDJIi/JFRUKn2J7tlTPd29SczMxMFBQWwF6nem5DAnw0iInJt7f449Oyzz8by5cu5XoyITsnKfYXmYh7eBlw4oIvW3XEKw7qF4YM/MtQI2fEUV9bhq3XmoHt70zqx49HrPHDbmd3x6Lfb8MHvGbhhVDd4GXRtDmMpqamorrJflUZfPz/sSktjKCMiIpfW7kB2/vnn45FHHsG2bdswdOhQ+Pv7t7j/L3/5iy37R0Qu6pOmPbBkpEem41HbC3uk55ajtKoewX6ex7T5dNVBVNc3om9sEE7vaV7jeyJSTGXG4t1qbdoPW47giqHHrjdrjYyMSRibPO1VRCecOPh1RG7mPsx5eap6HgYyIiJyZe1+F3TnnXeq8xkzZrS6JqGxsdE2PSMilyVT7n5Jy4XOA5gyOlHr7jiNiABvtZ/Y3rwKte7rnrN7tbi/uq4Rs1eZg+7fz+qh/k8+GR9PPf42uhte/Tkdn60+2OZAZiFhLK5X33Z+JURERNThKotGo/G4J4YxIjoZKdn+jx/T1OWrTotHz6g/y7HTyd0z3lyk461le7E/v6LFfVLCvqiyDvFhvrigX0ybj/nXYfGqguPmrBLsPFJm8z4TERGRDQNZczU1NafycCJyQwu352BTZonarPj+Cb217o7T+cvAWIzpFYG6BiMe/267dU+ylXsLrEH31jHdYdC3/b/3yEBvTOxrDnCfrz1op54TERGRTQKZjII9//zz6Nq1KwICArB//351+5NPPokPP/ywvYcjIjciJe5fXrhLXb51TBKig3y07pLTkWmIL17SHz6eOqzaX4hvNhzC6v2FuGn2OrWn2PiUKFwzvP1rriyVLr/fdASVtQ126DkRERHZJJC9+OKL+OSTT1T5ey8vL+vt/fr1wwcffNDewxGRG1m0rwoHCqsQEeCF205SAZCOLyHczzq6+MKCNNz0yTrU1BsxNjkS7103BJ7tGB2zGNU9HN3C/VBR24B5W47YoddERETUmnb/1f7000/x73//G5MnT4Zer7fePnDgQOzaZf7km4joaB5efvh6p3nN030TenMT4lN08xlJSO0ShNLqelTVNappjLOuGwpvw5//L7eHTudhHVn7fG2mjXtLRERENgtkhw8fRs+e5kXlzUlRj/r6+vYejojcRNg5t6Os1ojuEf64+rR4rbvj9GQU7JXLByDUzxMTUqPw/g3DVMXEUyEVFr30Omw9VIrth0tt1lciIiKyYSDr06cPfv/992Nu/+abbzB48OD2Ho6I3EBmpQ4B/carMvevXjmgQ1Pq6Fj944Kx9vEJ+GDKaaccxkR4gDcmNlVnnLOGo2RERESdod1zhp566ilMmTJFjZTJqNi3336L9PR0NZVx/vz59uklETmtsup6bCoyh4UrUgMwNDFM6y65FFuH28kjEtQasv9tPoxHzktpdfNpIiIisp12/yW/+OKLMW/ePPzyyy/w9/dXAS0tLU3dds4559iwa0Tk7IwmE37ekYMGkwdqDqfhyj4BWneJTmJEUhhSYgLVurT/rDZvMk1ERET206GPVseMGYPFixcjLy8PVVVV+OOPP3DuuefavndE5NTWHyjGkdIaGDxMKJz3GvQyZ5Ecvqz+7U0VMD9ecQDVdY1ad4mIiMiltTuQZWVl4dChQ9bra9euxf33368qLxIRWWQWVan9scSgsEY0lOZq3SVqowsHdEFcqC8KK+vw9fosrbtDRETk0todyK699losW7ZMXc7JycGECRNUKHv88cfx3HPP2aOPRORkymvqsXB7DkwA+sYGIdHfqHWXqB0Meh3+fmZ3dfnfv+1HfSNfPyIiIocJZNu3b8fw4cPV5a+//hr9+/fHypUrMWfOHLVhNBG5twajEQu2ZaO6vhFRgd4Y2ztS6y5RB1w5LF5t4H24pJobRRMRETlSIJO9xry9vdVlKezxl7/8RV1OSUlBdna27XtIRE7lt90FyC2rhY9Bh0n9u6jRFnI+Ukb/xtOT1OX3ft0Ho1HGO4mIiMjW2v1OqW/fvpg1a5bai0wKe5x33nnq9iNHjiA8PLxdx3rmmWfUAvLmJwl2FjU1NbjrrrvUcQMCAnD55ZcjN7flOpTMzExMmjQJfn5+iIqKwtSpU9HQ0NCiza+//oohQ4aoICmbWrc2kvfOO++gW7du8PHxwYgRI9Q0TCJqn/Sccmxr2lBY9rMK8mXJdGd2/ahEBHobsCevAr+kcQ0gERGRQwSyl19+Gf/6178wduxYXHPNNRg4cKC6/YcffrBOZWxvwJORNctJKjZaPPDAA6qc/ty5c7F8+XIV+i677DLr/Y2NjSqM1dXVqWmTs2fPVmFLSvFbZGRkqDbjxo3D5s2bVQGSW265BT///LO1zVdffYUHH3wQTz/9NDZu3Ki+pokTJ6oqkkTU9nVjy9LNvzPDu4WhW7i/1l2iUxTk44nJIxPV5beX7YXJxFEyIiIizTeGliBWUFCAsrIyhIaGWm+/7bbb1ChVuztgMCAmJuaY20tLS/Hhhx/i888/x/jx49VtH3/8MVJTU7F69WqMHDkSixYtws6dO9XUyejoaAwaNAjPP/88pk2bpkbfvLy81GheUlISpk+fro4hj5fQ9/rrr6vQJWbMmIFbb70VN954o7ouj1mwYAE++ugjPPLII+3+mojcjbxR/yUtD7UNRkQHeWN4Ejd/dhW3jEnC7JUHsPVQKX7dnY9xyVFad4mIiMildGhxh16vbxHGhEz3kymD7bVnzx7Exsaie/fumDx5spqCKDZs2KDWq0kVRwuZzpiQkIBVq1ap63IuRUUkjFlIyJKwuGPHDmub5sewtLEcQ0bX5Lmat9HpdOq6pU1ramtr1fM0PxG5qy2HSlWZe4POAxP7xHC/MRcSEeCN60YmqMszf9nDUTIiIiKtApkEsLCwsGNOMvokAUfWk7WXrNWSKYYLFy7Ee++9p6YXyqbT5eXlqqS+jHCFhIS0eIyEL7lPyHnzMGa533LfidpIgKqurlajfTL1sbU2lmO05qWXXkJwcLD1FB8f3+6vn8gVFFXW4Y+9BeryGT0jEOrvpXWXyMZuO7MHvA06bM4qwe97zK81ERERdfKUxTfeeKPV20tKStQI04UXXohvvvkGF110UZuf/Pzzz7deHjBggApoiYmJqpy+r68vHNmjjz6q1p1ZSMBjKCN3I6Mli3fmotFoQkKYHwbEBWvdJbKDyEBvTB6RiI9WZGDmkj0Y0ytC6y4RERG5XyCbMmXKCe+X9VsyatSeQHY0GQ3r3bs39u7di3POOUdNJ5TA13yUTKosWtacyfnR1RAtVRibtzm6MqNcDwoKUqFPpl/KqbU2ra1ts5CKjZby/0TuSqrv5ZTVwFPvgXNSo1WlVHJNt5/VHXPWHMSGg8VYua8Qjv2RGRERkfOw2QZBMkK2a9euUzpGRUUF9u3bhy5dumDo0KHw9PTEkiVLrPenp6erNWajRo1S1+V827ZtLaohytRJCVt9+vSxtml+DEsbyzFkWqQ8V/M2RqNRXbe0IaLWN4Be0TRVcWhiKAJ82l0jiJxIVJAPrhnOtWREREQOG8ikyIWEm/b4v//7P1XO/sCBA6ps/aWXXqpGq6ScvqzLuvnmm9W0wGXLlqlpkVIFUUKSVFgU5557rgpe119/PbZs2aJK2T/xxBNq7zLL6NXtt9+O/fv34+GHH1aB8d1331VTIqWkvoU8x/vvv6/K5qelpeGOO+5AZWWlteoiER1r26FSlNU0wM9LjyEJLYv8kGu6Y2wPeBl0WHugCHuL67XuDhERkUuw2UfaUqJepi22x6FDh1T4KiwsRGRkJM444wxV0l4uCylNLxUPZUNoCXxSPEQClYWEt/nz56sAJUHN399fTa187rnnrG2k6IiUsJcANnPmTMTFxeGDDz6wlrwXV111FfLz89X+ZVLIQ74OKTRydKEPIjKrrW/E2owidXlU93B46m322Q45sOggH5zfLwb/23wESzOqte4OERGRewWy5gUsjt4vTDZT3r17N3777bd2PfmXX355wvt9fHzwzjvvqNPxSBGQH3/88aR7p23atOmEbe6++251IqKTW3ewGDUNRoT5eaFPlyCtu0Od6Kph8SqQ/Z5ZDQ8D19ESERF1WiA7XqCR9VpSgOPbb79Vo1FE5NrKa+pV+XNxes9w6LjnmFsZ2T0c8WG+yCqqhl/yaK27Q0RE5D6BTNZxERFJlT0pc981xBdJEf5ad4c6mQTwK4fGY8bi3QgYcK7W3SEiInJ6LItGRO1aO7Yzu0xdPq1baLvL3EvRHFuzxzFdiVSmLSiw7WbOKd6N8IAJPgn9UVFfZ9NjExERuRsGMiJqs+1HylDfaEK4v5faCLqtyory1fl1111nt77Jthl0bBhLSU1FdVWVzY8ddeUz8O0+DHuKG5Fi86MTERG5DwYyImoTmaZoWTs2OCGkXaNj1RXmUbVJf38cyQOG2rRfaWuX46fZM1FTU2PT47oCGRmTMDZ52quITuhh02Ov2ZKGQ1Itt9YLRqOJawmJiIg6iIGMiNpkb14FKmob4OupR3J0YIeOER6biLhefW3ar9zMfTY9niuSMGbr73t25j4crCpFnV8wDhZVcT0hERFRB7Vp86AhQ4aguLhYXZY9vqrsMP2FiByXyWTCpizz/wED4oJh4L5jbk9+Aip3mIs97WpaV0hERETtp2vrovnKykp1+dlnn+VaDSI3k11ag9yyWuh1HiqQEYnKXb+r8wNFVWpKKxEREdlpyuKgQYNw44034owzzlCflL/22msICAhote1TTz3VgW4QkSPblGleOyZTFf28ONOZzOqy98ATjahrkNBejbjQthd6ISIiIrM2vbP65JNP8PTTT2P+/PlqIf9PP/0Eg+HYh8p9DGRErrcR9L78CmsxDyIrkxFh+lrkNvrhQEEVAxkREZG9AllycjK+/PJLdVmn02HJkiWIiorqyPMRkZPZcaQMMhlNNoKOCPDWujvkYML0NSqQZRRW4oxeEVp3h4iIyOm0e+6R0Wi0T0+IyOFIOXMJZKJ/V64do2PJCJnsgFBUWYey6noE+Xpq3SUiIiKn0qHFIPv27cMbb7yhin2IPn364L777kOPHrbd54aItCWjHpZS9z2iWNacjmXwMCE22BeHS6qRUVCJgfGc1kpERNQe7a5d/fPPP6sAtnbtWgwYMECd1qxZg759+2Lx4sXtPRwRObBth0vVeZ8uQTDoWOqeWtctws8a4ImIiMjOI2SPPPIIHnjgAfzzn/885vZp06bhnHPOae8hicgByfSzg4XmPQf7dQ3SujvkwJLC/bFibyEOFVejvtEIT+5TR0RE1Gbt/qsp0xRvvvnmY26/6aabsHPnzvYejogc1PYj5tGx+FBfhPh5ad0dcmBh/l4I9DGovciyiswhnoiIiOwUyCIjI7F58+ZjbpfbWHmRyDXIG2sW86C2ki1PZJRMcNoiERGRnacs3nrrrbjtttuwf/9+jB49Wt22YsUKvPzyy3jwwQfbezgickD7CypQVdcIPy89uke2vgk8UXPdIvyx9XCp2o/MZDKpkEZERER2CGRPPvkkAgMDMX36dDz66KPqttjYWDzzzDO4995723s4InJAWw+Zpyv2jQ2CXsc31nRyMrXVoPNQVTkLK+vcfs+6zMxMFBQU2OXYERERSEhIsMuxiYjICQKZfOopRT3kVF5erm6TgEZErqGwolYVZ5AYxumK1FYGvU5tHn6wqAqZRVVOHchk/z0Z4OvoKJ+EsZTUVFRX2Wc9na+fH3alpTGUERG58z5kFgxiRK5nS9PoWPdIfwT6cJNfarv4MD8VyKSwx5CEUDijPbnluPTdlfD10mNk93CM7hGOMb0iEBdqLu3fFjIyJmFs8rRXEZ1g2/05czP3Yc7LU9VzMJAREbmGUwpkRORa6o3ArhxzMY+Bcdzgl9onPsxXncsm0VIYxhmnu36+NlNNu5TTvC1H1Em+jv/ddTr6tXPEWMJYXK++dusrERG5Bm4WQ0RWByt1qG80qTLmcaHmN9dEbRUZ4A0fT/PPUG5ZDZyNhMgFW7PV5akTk3Hv2b3QLdxP3S7BjIiIyB44QkZETTywr1yvLg2MC2aVvE4k+zs603GPR35mZGrf3rwKNW0xNsS5Qv26A0XIK69Ve6rdMiYJ3gY9kqMDcdfnG7E4LRePXpCqdReJiMjdA1l9fT3OO+88zJo1C7169bJfr4io0/l0G4SKBg946XVIiQnSujtuoawoX51fd911dn2eiooKdJYESyArrsYIOJf5W82jYOf1jVFhTJzZOwKeeg/sz6/EvvwK9OA2EEREpGUg8/T0xNatW23dByJyAIFDLlTnfboEwcvA2cydobrCvF5v0t8fR/KAoTY/ftra5fhp9kzU1NR0+jqy7NJq1Dca4al3jp+lhkYjftqWoy5fODDWersUtpHiHr/vKcCStFwGMiIi0n7KonyS++GHH+Kf//yn7XtDRJo4Ut4A356nqcsD4ljqvrOFxybapfiDVOTrbMG+nmrKX3lNA46UVCMx3B/OYPX+IrV/Wqifp6qs2Nw5faJVIPtlZx5uO9O2VROJiIjaHcgaGhrw0Ucf4ZdffsHQoUPh79/yj+2MGTNs2T8i6gQ/pFfCw0OHGB8jQv29tO4OOTFZRxYf6oed2WVqPzJnCWTW6Yr9uhwzqnd2ajSe+t8OrD9YhKLKOlX0hoiISLNAtn37dgwZMkRd3r17d4v7WASAyPnklddg2QHzBrbJQY1ad4dcgExblEAm68icQV2DET9tN09XvGhAl2Pulw2vZSqvfE3LduXh8qFxGvSSiIhcVbsD2bJly+zTEyLSxOyVB9T+Y7WHdyE8vrvW3SEXICNkIr+8FtX1jfD1NBfIcFQr9hagtLoeEQHeGNG95XRFiwl9olUg+yUtl4GMiIhsqsOrrffu3Yuff/4Z1dXmT0BNJpMt+0VEnUA2v/3PqoPqcuma/4KD3GQL/t4GhDdN6ztUZB59dWTzmqYrXtA/5ribWZ+TGq3Ol+/OR009R5KJiEjDQFZYWIizzz4bvXv3xgUXXIDsbPMmmjfffDMeeughG3aNiOzty7WZKKtpQGygHtV7VmvdHXIh8WHmUbLMYscOZPJh4m+7zdsPXND/2OmKFv26BiE6yBtVdY1Yvb+wE3tIRESurt2B7IEHHlDl7zMzM+HnZ/6DK6666iosXLjQ1v0jIjuum/nwjwx1+eJkKeXNUW6yffn7zMIqh55BIRtBF1TUQQbGBsaFHLedrJGe0DRKJtMWiYiINAtkixYtwssvv4y4uJZz6GWj6IMHzVOfiMjxzdtyBNmlNYgM9MbYRPObZyJbriPTe3ioEdiSqno4qu2HS9W57C/m63XitW6WQPbb7oJO6RsREbmHdhf1qKysbDEyZlFUVARvb29b9YuI7EhGLD5oGh278fRu8NSbNygmshUpHR8b6oOsomocKKx0uO0UZJZHQUEBFu8oV9e7+DRg48aNJ3yMvs5ofmxRFZavWo9A79Y/00xLS7NDj4mIyFW1O5CNGTMGn376KZ5//nnrNA6j0YhXXnkF48aNs0cficgOm+CmZZep6nfXDk/A/l3bte4SuaBu4f5NgawKgxNC4UhhLCU1FdVVVYi85DH4JY/G95+8jf/c9f1JHxt72/vwDO2C8665BTUHt5ywbUVFhQ17TURErqrdgUyClxT1WL9+Perq6vDwww9jx44daoRsxYoV9uklEdnURyvMo2OXD+2KED/HGrkg1wpkv+8pwOGSatQ3Go/ZcFkrMjImYWzytFexUZ+Cqkbg4qunIOpvN5z0sasL9DhcBZx9+7NIDjKPmB0tbe1y/DR7JmpqauzQeyIigrsHsn79+qkNod9++20EBgaqTwAvu+wy3HXXXejS5fgVqojIMRwsrLQWJfjb6CStu0MuLNTPE4E+BpTXNOBQcTWSIvzhSELjeqDqkLnMfZ+UZPi0Yb+0bp5FOLyvELVeIYjr1frfvNzMfTbvKxERua52BzIRHByMxx9/3Pa9ISK7+2TlAUjRu7HJkegZJdUViexDprTLKNm2w6VqHZmjBbKSOnMYC/IxtCmMiaggH2t1RiIiIs0CWXFxMT788EPrwuU+ffrgxhtvRFhYmE06RUT2UV5Tj7nrD6nLN53O0TGyv27hfiqQHWwqfy8hzdECmVQabauopral1fVqg+i2BjkiIqLjafeE/t9++w3dunXDm2++qYKZnORyUlKSuo+IHNfX6w+horZBjYyN6RWhdXfIDcQ1lb+XAFNS7Vjl7y2BLCrQPOrVFhLAZERNcJSMiIg0CWSyVkw2gc7IyMC3336rTvv378fVV1+t7uuof/7zn+qT0/vvv996myyIlmOGh4cjICAAl19+OXJzc4+pljVp0iRVij8qKgpTp05FQ0NDiza//vorhgwZosry9+zZE5988skxz//OO++ooOnj44MRI0Zg7dq1Hf5aiBxRo9GET1ZmWEfHHGmkglyXl0GH2BBz4DlQUAlHUlLf/hGyFtMWy1i0g4iINAhke/fuxUMPPQS9/s9pGnL5wQcfVPd1xLp16/Cvf/0LAwYMaHH7Aw88gHnz5mHu3LlYvnw5jhw5ogqIWDQ2NqowJtUeV65cidmzZ6uw9dRTT1nbSHCUNlKSf/PmzSrw3XLLLfj555+tbb766ivV/6efflrtQzNw4EBMnDgReXl5Hfp6iBzRmoxCVYI82NcTlw7uqnV3yI3IOjIh5e8dhYfBG+UdDWRN7TlCRkREmgQyGWlqbdNLuU2CTHtJlcbJkyfj/fffR2jon/vUlJaWqnVqM2bMwPjx4zF06FB8/PHHKnitXr1atVm0aBF27tyJzz77DIMGDcL555+v9keT0S4JaWLWrFlqOuX06dORmpqKu+++G1dccQVef/1163PJc9x6661qHZysh5PHyIjbRx99dNx+19bWoqysrMWJyJHN35qtzs/vFwNfL657oc6TGO6nzi3l7x2BZ2SixDK1F59/O38folnYg4iIOjuQbd261Xq69957cd999+G1117DH3/8oU5yWUaz5NReMiVRRrAmTJjQ4vYNGzagvr6+xe0pKSlISEjAqlWr1HU579+/P6Kjo61tZGRLwpHsjWZpc/SxpY3lGBLc5Lmat9HpdOq6pU1rXnrpJVVt0nKKj49v99dO1FkaGo1YuD1HXZ40gNtTUOcK8/dS5e9l2mxmkWOMknlF91DnUUHe7Z6+e3RhDyIiIrtXWZTRJ/mDJRWyLGRD6KNde+21an1ZW3355ZdqiqBMWTxaTk4OvLy8EBIS0uJ2CV9yn6VN8zBmud9y34naSGirrq5WRUlk6mNrbXbt2nXcvj/66KNqmqOFHI+hjBzV6v1FKKqsU2+MR3UP17o75Gbk70evqABszCzBruxy9IgMcJhAFhnQvumKzQt7lNU0qFGyhDDzCCAREZHdApmsw7K1rKwsNdK2ePFiVUjD2UiBEDkROYMF246o8/P6xcCgb/dMZaJTlhITpAJZRkGlQ5SL94ru3mK0q72ksEdZTQXyymsYyIiIyP6BLDFR5trblkwTlKIZsibNQkaqpHT+22+/rYpuyHTCkpKSFqNkUmUxJiZGXZbzo6shWqowNm9zdGVGuR4UFARfX19VkEROrbWxHIPImcmanZ+apite2J/TFUkbUjgjIsALBRV12JNbgf5xwZr1pcFogldkN2u/OkKC3N68CuSVcR0ZERFpsDG0VDuUtWMSqIzGlgu0ZY1ZW5x99tnYtm1bi9ukqIasE5s2bZqa/ufp6YklS5aocvciPT1dlbkfNWqUui7nL774ouqHlLwXMuImYUuKc1ja/Pjjjy2eR9pYjiHTIqVgiDzPJZdcom6Tr0muSwEQIme3cl8hSqrq1Zvh4UncvJ20kxoThN/3FiAtp0zTQHa4rAEeBi8YPEyq6mhHsNIiERFpFsikrPzf//53FWRkf7Dmi6HlclsDWWBgIPr169fiNn9/f3VMy+0333yzWqcVFhamQtY999yjgtTIkSPV/eeee64KXtdffz1eeeUVtV7siSeeUIVCLNMJb7/9djXiJmvebrrpJixduhRff/01FixYYH1eeY4pU6Zg2LBhGD58ON544w1UVlaqgEjk7BZs5XRFcgzJMYH4Y28BsktrUFJVhxA/L036sb/EvEF1iJepw/vxWfYisxT20HoKJhERuVEge/LJJ9U+X1LUQqoR2pOUppfnkBEyKTMv1RHfffdd6/0y1XD+/Pm44447VFCTQCfB6rnnnrO2kZL3Er6kAuTMmTMRFxeHDz74QB3LQgqR5Ofnq69LQp0UMVm4cOExhT6InE1dgxE/7zBPx71wQKzW3SE35+9tUOutDhZVYVdOOUZqVGDmYEmDOg/2/LNQVXtJuXwW9iAiIk0CWVVVFa6++mq7hLFff/21xXUp9iF7isnpROvbjp6SeLSxY8di06ZNJ2wj0xM5RZFczYq9BeoTfFknc1o3Tlck7aV0CbQGshFJYR0eoToVB0vNI2TBXh0PZM0Le+SWsbAHERF1XLtTlUwjnDt37ik8JRF1lp+2mzeDvqBfDPS6zn/jS3Q0KXnvqfdQHxTI1EUtHCw99REyEdM0bVECGRERUaeNkMmGyBdeeKGa0iebMkvhjeZmzJjR4c4QkW2t2Fuozs9O5fRbcgyeeh16RgUgLbscadlliA3x7dTnL6yoRUmNESaTEUE2CmQ5DGRERNTZgUxK0icnJ6vrRxf1ICLHkFVUhcMl1TDoPDCsW6jW3SGy6tMlyBzIZNpi93AEeHeo4G+HpOeUq/OG4hwYEiNO6VhRQd6QP3uVtY0or6lHoE/HKjYSEZF7a/dfwenTp+Ojjz7C3/72N/v0iIhsYtV+8+jYwPgQ+Hl13hteopPpGuKLLsE+asri2owijE8xb1vSGSQEivr8AwAiTnm0L9zfvLeajJIxkBERUaesIZNy8qeffnqHnoyIOs/qpkA2sjuLeZBjkdkUp/cwh6EdR0rVerLOkp5Tps7rVCA7ddZ1ZKXcj4yIiDopkN1333146623Ovh0RNQZTCYT1uwvUpe1Ki1OdCJdQ32RGO4Ho+nPDw86g1R3/HOE7NTFBHMdGRERnZp2z2Nau3at2lxZ9v/q27fvMUU9vv3221PsEhGdqkPF1db1Y0MTuX6MHNOo7uE4WGgugS8/pxEB3nZ9vkajCbtzy+0zQlZWA6PRBB2rmRIRkb0DWUhICC677LL2PoyIOhHXj5EziA7yURUX9+ZVqFEye29efrCwEjX1RnjpgYaSHJscM9TfC156HeoajSisrFN7/hEREbVHu9+pffzxx+19CBF1Mq4fI2caJduXV4F9+ZU4XFytpjLau8JiQrAn9piMNjmmzsNDVVuUUWmZtniqgay2oRH55bVq5M2gb/eqAiIickL8357IBdePrd5nDmSjup9aFTkiewvz90Lf2CB1+eedOSqQ2LvCYmKwbUeNrfuRdXCj67oGo9qTbd6WI3j/9wz8d+NhLEvPt2kfiYjIcbX7r1JSUtIJ9xvbv3//qfaJiE5BVlE1jpTWwFPvgSGJIVp3h+ikzugVgazialVtcemuPJzXN8auFRYTg21bnt5S2EPWkbVXfaMRX67LRHFVy0qTaTllGJEUhiBfltInInJ17Q5k999/f4vr9fX12LRpExYuXIipU6fasm9EdArTFQfGcf0YOQdvg16FsLkbsrA7twKJ4eUwj5nZp8KivUbIZA1Ze0f4NmeVqDDm66lH/7hg9IoKwG978tUHKxsyizEuufP2aCMiIm0YOlL2vjXvvPMO1q9fb4s+EZFN1o+x3D05Dxllkp/ZlfsK8Wt6HsbZOIdU1jYgs6hKXU6wcSDz9zYg0MeA8poG5JW1fT+y6vpGrD9QrC6f2TsCKTHmGHpaYhiyig5jx5EyDO8Wpo5PRESuy2ZryM4//3z897//tdXhiKij68cYyMhJSen7uBBf1DeasK7QAOhsF0Sk3L3JBEQFeiPYRw9bs64ja8e0xXUHilR1xogALyRHB1pvjwv1RXSQtyrTLyNoRETk2mwWyL755huEhbGiG5GWuH6MnJlULDy3bzR8DDoU1+kQMuY6m09XTI75M/hoWdijrLoeW7NK1eUzeka0WJstl0/rZv57uvVQqV0LnRARkfba/fHj4MGDW/zhkE/kc3JykJ+fj3fffdfW/SOidtiUZZ7+1Dc2mOvHyCkF+nji7NRoLNiWjeCRV2BLbi2G2LDkfWoXmRZYDVuLbirskV1ag5g27hXYaDIhPtQXCWF+x9zfPcJfVaAsqqxTocwS0IiIyPW0+x3bJZdc0uK6TqdDZGQkxo4di5SUFFv2jYjayTK9aVA8R8fIeclm0UkBjcio0OPNNSW4+Mw6FU5OhZSVF+apgdV2GSGTwhyyLqwIJx6Fk33GLCN2px81OmYhtw1LDMWinbnYlFmCwfEh3JeMiMhFtTuQPf300/bpCRGdMvkkXQyIC9a6K0SnZEBII9IPHEZxRAIe/mYL3r9h2Am3XDkRo9HUYspiXW6ejXsL6HUe6BMbhA0Hi5GDE38gsmJfgTrvHRWA6Kapjq3pHR2IFXsLUFnXqKYitzaSRkREzo8ftxG5CNnPaPthcyAbyBEycnIGHVDww6vq/Je0PMzdcKjDx0rPLVd7nPl76e22hkz0a9rgugT+MARHt9omq6gKBwuroPMARvUIP2nI6xrqqy5nl9p+VI+IiJwskMnURL1ef8KTwcA1K0RakSpytQ1GBHobkBTur3V3iE5ZfX4GrulnDlCv/ZyOqrqGDh1n1T5z5dFh3cLgacdpfyF+XogPkwDlgYAB5x5zv6y5/mOveXSsf9dg1f5kYoObAllJ+zedJiIi59DmBPXdd98d975Vq1bhzTffhNFotFW/iKij0xXjg6GTj9+JXMCFvfzx66EGVUH0g98zcO/Zvdp9jM7cCqJ/bLDqq/+Ac2A0tQxRe/MqkFdeq6qgDk9qW5GOLiF/FgsxSt1+IiJy30B28cUXH3Nbeno6HnnkEcybNw+TJ0/Gc889Z+v+EVEbbWkq6DEgjtMVyXVIeHl4Ygru+WITZi3fh6uHxyMq8PjrrlpbP7Ymo0hdHtnd/pUKu0cGwBMNQEAYChvNzytkT7EVTSN1QxJC21wFNcLfW30PZL+ywoo6u/WbiIi006G5G0eOHMGtt96K/v37o6GhAZs3b8bs2bORmJho+x4SUZtsaRohG8hARi7mwgFd1LrIqrpGvPHLnnY9Ni2nzLp+TKYJ2pus+4qG+cOR7IY/i3DsOFKq+iGVGCWQtZWMdsdYS+pzHRkREdw9kJWWlmLatGno2bMnduzYgSVLlqjRsX79+tmvh0R0UtV1jWoNmRgYzwqL5FqkuuLjF6Sqy1+ty8LePPPPelus3m8epTotKazTysbHNAWyYqMP5m89gs9WH8Ty3fnqthHdw+AllUrawbqOrI2bThMRkXNp81+FV155Bd27d8f8+fPxxRdfYOXKlRgzZox9e0dEbSKfvsuUqMhAb7UfEpGrkTVX5/aJVj/n//xpl0OuH7PwQT2qMzaqy/vyK1FYWQejCYgL8UW/2PZ/YNKlaYTsSInzjJC9snAX/jprFZak5apiJkREZIM1ZLJWzNfXV42OyfREObXm22+/beshicjGG0LLdMWO7tVE5OgeOT8FS3blqTL4mzKLMfgkU/8kvK1pCmSjOjGQiaJF72LEXW+ge1I3hPp7IczPC4E+hg79fsqURXlUWU0Dqhvh8HYeKcO7v+5Tl9ceKMLpPcPx+AV91D5tRER0CiNkN9xwA/76178iLCwMwcHBxz0RkXYVFgdyQ2hyYVIw47LBXdXltqwlS8suUyEmwNuAvp0cBhpKctDDq0yFxm7h/gjy9ezwhyXeBj3CA8wl8otqHf8Dl/eWm8NY9wh/eOl1WLG3EJPe+h1z1hzUumtERM49QvbJJ5/YtydE1GFbDjWNkHFDaHJxd4/viW83HVZrsjZmFp+wQIZluuJp3UI7bf2Yvcg6soKKOhTUOvbXcaCgEgu2HlGX37p2MIJ8PPHigjQs3JGDd5ftwxkxQGGh+XWxh4iICCQkJNjt+ERE9sCdnImcXElVHQ4WVqnLAzhCRi4uMdwflw/piq/XH1KjZJ/eNPykgWxUj86drmgPsh/Z1sOlKHTwEbJ//bZPrZcblxyJvk3r5WZcNRBLn83D4ZJq9Bt9NiqOmEfQ7MHXzw+70tIYyojIqTCQEbnIdMVu4X4I8TNPayJyZXeP64X/bjyM33bnY8PBYgxNDG19/Zh1/zHnD2SWSosldR7wMHjDEeWU1uCbDYfU5bvG9bTeLnuuDesWipX7CuERk4rJ19+O6IQeNn/+3Mx9mPPyVBQUFDCQEZFTYSAjcnLcEJrcTUK4n3WUbOaS1kfJpLBEeU0DAr0N6NPF+YtJSEEQf289Kmsb4RXzZ9hxJO//vh/1jSZVEXNYt5abcI/pFakCmU/SEBXG4nr11ayfRESOxrEnoxNRmzeE5nRFcrdRMoPOQ42SrW0aCWvui3WZ6lzCgbOvHxNSEMQySuYd1weOpqiyDp+vyTxmdMzizN4R6twnob+a0khERH9y/r9SRG5u22EW9CD3HCW7YmicunznnA3IKKi03vf1uixrOLhuVCJchWU/Mu+u5k2yHcl/NxxCdX0j+nUNwpm9zOGrudSYIIT46KDz8nX4dXBERJ2NgYzIieWV1yC3rBZSTdsVpmURtcejF6Sqn3upPnjdB2vUxsmypuyJ77er+x+Y0BvjkqPgKqKbNn33irb9+qtTtXRXnjq/Ykhcq+X9dToPDIw2r33LreFbDyKi5vi/IpET23G4zLrfj783l4SSewn29cSnNw9XP/9Swe+6D9fg9s82oK7RiPP6xuCe8Y651qqjIgMl0JhgCAxHkQPtEF1eU491B8zTRseeIAAPijEXHcqt5ggZEVFzDGRETmzbYfP6sf5duX6M3FNEgDf+c8sIxAb7YH9+JfLLa5EcHYjpfx2oRmVciadeh0BP8wKsjJJ6OIo/9hSgwWhSwbhbhP9x21lGyErqdaiqa+jEHhIROTYGMiIXCGT9GMjIjXUN8VWhLDrIG1GB3nj/hmEuO2Ic2hTI9hU5TiBblm6erjgu5cTTQ0N89KjLNe9Blllk3juRiIhY9p7IqW3nCBmR0iMyAL89PA5GI+DrpYerCvEyIbMK2O8gI2RGownL0vPV5bas16vO2KjWwGUWViElhuteiYgER8iInFRBRS2yS2vU5b4MZETwNuhdOoxZApnYX+wYgWxndpmaJurnpcdpScdu0N1aIBMHi6pgMrH+PRGRYCAjcvLRMVm3EeCi07OIqPVAVlBlRGFFrdbdwbKm6opn9IxQgfhkag+nQe9hQlVdo6qOSUREGgey9957DwMGDEBQUJA6jRo1Cj/99JP1/pqaGtx1110IDw9HQEAALr/8cuTm5rY4RmZmJiZNmgQ/Pz9ERUVh6tSpaGhouVj4119/xZAhQ+Dt7Y2ePXvik08+OaYv77zzDrp16wYfHx+MGDECa9euteNXTmS7QMb1Y0Tuw1MH1BceUpe3HzFXWdXS0jauH7NqbEBYU6iUbTuIiEjjQBYXF4d//vOf2LBhA9avX4/x48fj4osvxo4dO9T9DzzwAObNm4e5c+di+fLlOHLkCC677DLr4xsbG1UYq6urw8qVKzF79mwVtp566ilrm4yMDNVm3Lhx2Lx5M+6//37ccsst+Pnnn61tvvrqKzz44IN4+umnsXHjRgwcOBATJ05EXp75Dw2RI2KFRSL3ZCmMYflQRitFlXXYnGXemH5scmSbHxdsGeXjCBkRkfaB7KKLLsIFF1yAXr16oXfv3njxxRfVSNjq1atRWlqKDz/8EDNmzFBBbejQofj4449V8JL7xaJFi7Bz50589tlnGDRoEM4//3w8//zzarRLQpqYNWsWkpKSMH36dKSmpuLuu+/GFVdcgddff93aD3mOW2+9FTfeeCP69OmjHiMjbh999JFm3xuik9netAcZR8iI3Etd7l6HCGS/7c6HLANL7RKELsG+bX5ccFOlSFkHS0REDrSGTEa7vvzyS1RWVqqpizJqVl9fjwkTJljbpKSkICEhAatWrVLX5bx///6Ijo62tpGRrbKyMusom7RpfgxLG8sxJLjJczVvo9Pp1HVLm9bU1taq52l+IurMT6ZlI1zRtysrlRG5k7qcphGyI9oGsqVN68fGtWN0rOUIWS0LexAROULZ+23btqkAJuvFZHTsu+++U6NUMr3Qy8sLISEhLdpL+MrJyVGX5bx5GLPcb7nvRG0kQFVXV6O4uFiFwdba7Nq167j9fumll/Dss8+e4ldP1D6yZrKgoACbc8yfLHcJ0GPvzm2nfNy0tDQb9I6IOkNt05TFrKJqlFTVIcTPq9P70Gg0Yfnu/PatH2sSZDBBtuyuqTeq4h6uumccEVFbaf6/YHJysgpfMkXxm2++wZQpU9R6MUf36KOPqnVnFhLw4uPjNe0TuX4YS0lNRXVVFYJGXonQs6Zg77plGPrkKzZ7joqKCpsdi4jsw1RbiWh/PXIrG7HjSBlO7xnR6X3YnFWM0up6BPt6YnB8yw9OT0avA0L8PFFcVa9GyRjIiMjdaf6/oIyCSeVDIevE1q1bh5kzZ+Kqq65S0wlLSkpajJJJlcWYmBh1Wc6ProZoqcLYvM3RlRnlulR19PX1hV6vV6fW2liO0Rqp2Cgnos4iI2MSxiZPexUZvsk4XA2MGDUayRO/PeVjp61djp9mz1Qj1UTk+HqEeqpAJuvItAhklumKZ/aOhEESVjtFBHg3BbI6JIb726GHRETOQ/NAdjSj0ajWZ0k48/T0xJIlS1S5e5Genq5GCWSKo5BzKQQi1RCl5L1YvHixClsy7dHS5scff2zxHNLGcgwJhPJc8jyXXHKJtQ9yXQqAEDma6IQe2JonU5Qa0DspEXFhfqd8zNxM8xQoInIO3UM9sfJQjbXaamdbtiu/Q+vHLMIDvLAnDw6xlxoRkVsHMpn2J5URpVBHeXk5Pv/8c7VnmJSkDw4Oxs0336ymBYaFhamQdc8996ggNXLkSPX4c889VwWv66+/Hq+88opaL/bEE0+ovcsso1e333473n77bTz88MO46aabsHTpUnz99ddYsGCBtR/yHDJVctiwYRg+fDjeeOMNVVxEqi4SOZq6RqCsxrzXXmQgR2mJ3DWQCZmy2NlySmuwM7sMHh7AWb07FshkhEyw9D0RkcaBTEa2brjhBmRnZ6sAJptESxg755xz1P1Sml4qHsoImYyaSXXEd9991/p4mWo4f/583HHHHSqo+fv7q2D13HPPWdtIyXsJX7KnmUyFlL3PPvjgA3UsC5kemZ+fr/Yvk1AnJfQXLlx4TKEPIkdQXC/L4aHWbvh46rXuDhFpGMgyCipRVlOPIB/z9c7wa9Nm0APjQhDeFKw6GsikYqwUCNHrzP+vERG5I00DmewzdiI+Pj5qTzE5HU9iYuIxUxKPNnbsWGzatOmEbWR6IqcokjMorjW/cYni6BiR2wry1qFriK/a/mLboc5dR7asKZCNS25fdcXmgnwM8NR7oL7RpCpFdjTYERG5AofZh4yI2qa4zvxrGx3ko3VXiEhDw5PC1PnKfQWd9px1DUb8scf8fONSOjZdUXh4eCDcn9MWiYgEAxmRkymuM4+QxTCQEbk1y6iYJSB1hnUHilBZ16imHPaLDT6lY0UEmPdPK6xkYQ8icm8MZERORO8fiupGD7WpKgt6ELm3M5oC2dbDpSitqu+U51zWVO5+bHIkdKe47ouFPYiIzBjIiJyIV5de6jzM3wteBv76ErmzmGAf9IwKgMkErNpf4DTrx44NZBwhIyL3xnd0RE7Eq0tvdR4VxNExIvpzlOyPvfYPZJmFVdiXX6kqIo7pfepFRGQvMlFe04DahkYb9JCIyDkxkBE5Ee8Y8wgZC3oQUfN1ZCv2Ftr9uRan5arzYYmhNimzL9t2BHibiz1z2iIRuTMGMiInYTKZrFMWGciISIzoHqZGrGQ/skPFVXZ7Htkr7NNVB9TlC/p3sdlxrYU9OG2RiNwYAxmRk8itbITeNwgeMFnfxBCRe5ORqoFx5mqHK+04SvbzjhwcLKxCiJ8nrhwWZ7PjWvYf4wgZEbkzBjIiJ7G3yFxFLcTLBIOOv7pE1HId2e92Wkcmo/P/Wr5PXb5hZCL8vMzTDG3B8uESC3sQkTvjuzoiJwtkoV4mrbtCRA64jmzl3gIYjbb//2H1/iJsOVQKb4MOU0Z3s+mxLZUWCyvqVPAjInJHDGRETmJvMQMZER1rcEIofD31KKysw66ccpsf/1+/mUfH/jos3jrF0FZC/bwg25nVNRpVtUUiInfEQEbkBGRB/f6mQBbGQEZEzciehFLcQ6yw8bTFtOwy/Jqer0LTLWOSYGtSkET2VRSctkhE7oqBjMgJ7M2rQE2DCca6agR6MpARUevryBZsy7bptEXL2rHz+3dBYrg/7IGFPYjI3dluZS4R2c2WQyXqvC5nLzx6JmvdHSJyMOf1i8H0RbuxOasE7y3fh7vG9Wy1XU19I97/bb/aUyzQx4DIAG+1jqtPbBDOTo1GsK95f7H9+RV4YUEalu7KU9dvP7OH3fouhT3SWfqeiNwYAxmRE9jaFMhqc/YAYCAjopbiQv3w7MV98fA3WzFj8W6M7B6GoYnmaYwWy3bl4Zl5O1T5+tZ46j1UgZCuIb74en0W6huloqsH7ju7F/o3lda3hwh/jpARkXtjICNyAlsPlarzumwJZETuIy0tzSmO6QiuHBqn1pD9b/MR3PvFZvx47xg1CrZyXyE+XpGBJU2jXVGB3rh/Qm/4eulQUF6HnLIa/L4nH7tzK9R6MYuxyZF48sI+6BEZYNd+WyotFlfXoaHRCIOeqymIyL0wkBE5OJlitPNImbpce0Qm9hC5vrIiczC47rrr7PYcFRUVcCUeHh544ZJ+atqijILd8PFaFFXWIquoWt0vo103nt4N903ojQDvY//8780rx0/bcrAnrwKXDI7F+JToTum3v7deldSvbTCiqKoOUYE+nfK8RESOgoGMyAlGxxqMJoT56nCwzPwJN5Grq64wfwgx6e+PI3nAUJseO23tcvw0eyZqamrgagJ9PPH2NUNw2XsrsCXLPNU50NuAiwfH4m+ju6FnVOBxHyv33XP28e+3Z5CUUbLDJdVqPzIGMiJyNwxkRA5uw8Fidd473AubtO4MUScLj01EXK++Nj1mbqa5cqCrkvVe0/86CPO2HMHEvjGY1L8LfL30cGRS2EMCGUvfE5E7YiAjcpJAlhJurn5GRHQyfxkYq07OwrKOjIU9iMgdceUskQMzmUzYlGkOZMnh5s1TiYhczZ+BjCNkROR+GMiIHJgszC+srIOXXofuoRwhIyLXFOZv/sCpqq4RVXUNWneHiKhTMZAROcF0RVkTInsEERG5Ii+DzroptRT2ICJyJwxkRA5sY9N0xSEJIVp3hYjI7oU9BKctEpG7YSAjcoIRsqGJoVp3hYjIrsJZ2IOI3BQDGZGDKq+px+7ccnV5SAIDGRG5toimdWQcISMid8NARuSgtmSVwmgC4kJ9ERXEjVKJyLVFBJpHyKSQkdFk0ro7RESdhoGMyEFxuiIRuRMp6mHQeaDRaEJpdb3W3SEi6jQMZEQOX9CDgYyIXJ/Ow8Na/j6/nNMWich9MJAROSCj0WQNZBwhIyJ30SXYPD37cHG11l0hIuo0DGREDmhffgXKaxrg66lHSkyg1t0hIuoUcaF+6vwQAxkRuRGD1h0gomOtb1o/NjA+GAY9Pzchos6TmZmJgoICmx83LS3tpG2kiJEoqqpDZW0D/L35NoWIXB//pyNyQGv2F6rz4d3CtO4KEblZGEtJTUV1VZXdnqOiouK49/l46hEZ6K3WkMkoWTJnCBCRG2AgI3IwJpMJazKK1OUR3cO17g4RuREZGZMwNnnaq4hO6GHTY6etXY6fZs9ETU3NSUfJJJBlFVcxkBGRW2AgI3Iw8qlwdmmNKv/MCotEpAUJY3G9+tr0mLmZ+9rULj7UD5syS7iOjIjcBhenEDmY1U3TFQfEBcPXS691d4iIOlVsiA88PKD2Iiur4X5kROT6GMiIHMxaTlckIjfmbdAjOtBc/p6jZETkDhjIiByMdf1YEgt6EJF7slRbPFRkv+IiRESOgoGMyIFkl1Yjs6gKOg9uCE1E7ssSyLKKq1WhIyIiV8ZARuSA0xX7dQ1GoI+n1t0hItJEbIiv+mCqorZBrSUjInJlrLJI5EBW7+d0RSKyzSbLjnTc9vLU6xAT7IMjJTVqHVmIn5fWXSIics1A9tJLL+Hbb7/Frl274Ovri9GjR+Pll19GcnKytY3sV/LQQw/hyy+/RG1tLSZOnIh3330X0dHRLTayvOOOO7Bs2TIEBARgypQp6tgGw59f3q+//ooHH3wQO3bsQHx8PJ544gn87W9/a9Gfd955B6+++ipycnIwcOBAvPXWWxg+fHgnfTeIZISsaUPoJBb0IKJjlRXlq/PrrrvOrs9zos2bO0tcqJ8KZLIfmcwaICJyVZoGsuXLl+Ouu+7CaaedhoaGBjz22GM499xzsXPnTvj7+6s2DzzwABYsWIC5c+ciODgYd999Ny677DKsWLFC3d/Y2IhJkyYhJiYGK1euRHZ2Nm644QZ4enriH//4h2qTkZGh2tx+++2YM2cOlixZgltuuQVdunRRAU989dVXKrDNmjULI0aMwBtvvKHuS09PR1RUlIbfJXIXshHqvvxKVe55eDeOkBHRsaorytT5pL8/juQBQ21+/LZu3twZ4kN9sTYDOFhYhboGI7wMXGVBRK5J00C2cOHCFtc/+eQTFX42bNiAM888E6Wlpfjwww/x+eefY/z48arNxx9/jNTUVKxevRojR47EokWLVID75Zdf1KjZoEGD8Pzzz2PatGl45pln4OXlpUJWUlISpk+fro4hj//jjz/w+uuvWwPZjBkzcOutt+LGG29U1+UxEgQ/+ugjPPLII53+vSH3XT+WHB2IYD+uHyOi4wuPTbT5xs3t2by5s9aRhfh6oqS6HtsOl7LQERG5LIf6uEkCmAgLM48OSDCrr6/HhAkTrG1SUlKQkJCAVatWqety3r9//xZTGCVklZWVqemJljbNj2FpYzlGXV2deq7mbXQ6nbpuaXM0mT4pz9H8RGSL6Yojuf8YERF0Hh44rWm2wIaDxWhoNGrdJSIi1w5kRqMR999/P04//XT069dP3SZruWSEKyQkpEVbCV9yn6VN8zBmud9y34naSIiqrq5GQUGBmvrYWhvLMY4ma9RkCqXlJOvSiE4F9x8jImopOSYQQT4GVNc3YvsRx/ngs7SqHtsOlaKmvlHrrhCRC3CYKouylmz79u1qKqEzePTRR9WaMwsJdwxldCrrx3bllKvLwxnIiIgUvc4DwxLDsDQ9T42S9esaBINOm8+SF27PwaerDmBPXoX6P1vEBvtg9k3D0Ss6UJM+EZFrcIgRMinUMX/+fFUlMS4uznq7FOqQ6YQlJSUt2ufm5qr7LG3k+tH3W+47UZugoCBV3TEiIgJ6vb7VNpZjHM3b21s9vvmJqKNW7itQ5326BCE8wFvr7hAROYzU2EAEeBvUnmRpR8wfXHW2L9dm4o45G7ByX6E1jPl66nGktAZXzFqF9QfMMxyIiJwukJlMJhXGvvvuOyxdulQV3mhu6NChqlqiVEW0kKqHUuZ+1KhR6rqcb9u2DXl5edY2ixcvVgGpT58+1jbNj2FpYzmGTIuU52reRqZQynVLGyJ7+mOPOZCN6RWhdVeIiByKjIhZCnqsP1iERqOpU59fRsUe+XYbTCbg6tPi8f1dp2P7sxOx8pHxGJwQojaunvzBGiza0foSByIihw5kMk3xs88+U1UUAwMD1XotOcm6LiFrs26++WY1NVBGz6TwhlRBlJAkFRaFlMmX4HX99ddjy5Yt+Pnnn9UeY3JsGcUSUu5+//79ePjhh9WeZ7KP2ddff61K6lvIc7z//vuYPXu22hhT9jWrrKy0Vl0ksucHEyv2mgPZ6T0ZyIiIjtYvNkiNSJXVNGBTZnGnPe8Hv+/HU/8zFwi75YwkvHRZfwyKD1EjdqH+Xvj8lpE4OyUKtQ1G3P7ZBqzZby7ORETkNIHsvffeU5UVx44dq/YEs5xkTzALKU1/4YUX4vLLL1el8GUKoWwmbSFTDWW6o5xLUJPNMmUfsueee87aRkbepIS9jIrJhs9S/v6DDz6wlrwXV111FV577TU89dRTqnT+5s2bVVn+owt9ENna/oJKNe1F9tjh+jEiomMZ9DqM6mGuQLtiXyH259t/4+oFW7PxwoI0dfnOsT3w+KRUeMhGkc34eunxr+uHYlL/LpCBu5lL9ti9X0TkegxajwycjI+PD9555x11Op7ExET8+OOPJzyOhL5NmzadsI1Mn5QTUWeyjI4NSwyFj6de6+4QETnsKFleeQ22Hy7Dwh05uHJoPCID7bPmVtaJPfH9NnX51jFJmDox+Zgw1jwsPjYpFT/vyFFrzDZnlahRNCIipyrqQeTOfm9aP8bpikRExyeBaGzvKMSH+qK+0YR5W4+gsrbBLh8WP/7dNhRX1atCS1Mnphw3jFl0DfHFxYO6qsuzfnWczbWJyDkwkBFpSDY6Xb3PvOaABT2IiE5eBv+C/l0Q4ueJ8poGzN+ajQajbTeM/t/mI1i0Mxeeeg+8duVANZ28LW4/q7s6/3lnDvbm2X9KJRG5DgYyIg1tOVSK8toGBPt6om9ssNbdISJyeDK1+y8DY+Ft0CGnrAbL0/Ntduzcsho89b/t6vK943uhT2zbt7SRvcjO6ROtqjH++zeOkhFR2zGQEWnoz+qK4eqTXyIiOrlQPy+c3y8G8r/m9iNl2Ha41CZTFR/7dpuq5Ni/azDuGNuj3cewPOa7TYeRXWquGE1EdDIMZEQOsP8Y148REbVPYrg/RjdVXvw1PQ+Ftaf2odaCbdlYsitPTVWc/teBqlhHew1JCMWIpDC1xu3D3zNOqT9E5D40rbJI5M5kMfrGpv10xvSM1Lo7RERORzaMzi2vVWu2VhcYoPc3byDdXrK587PzdqrLd47tid7Rgce0yczMREGB+UO0EzknzoQ1GcDnaw7gnOhqFfDaIiIiAgkJCR3oPRE5OwYyIo2syShEg9GE+DBfJIT7ad0dIiKnI9UPz0mNRnFlHQor6xBx0VQ0yoZg7fTywl2q1H33SH/cOa5Hq2EsJTUV1VVVbekV4u7+FFX+oRh75c2oObilTX3w9fPDrrQ0hjIiN8RARqSRZbvMC9HP4OgYEVGHSRXESQO64PPVB+CTOAD/TavAacPa/vh1B4rw+ZpMdfmlS/vD23DsfpAyMiZhbPK0VxGdcPK1ZRsK9ThQCYy+5TkMCms8afvczH2Y8/JU9TwMZETuh4GMSAOyePyXtFx1+Zw+UVp3h4jI6Yt8DA5rxLpCA77eWYHLMoowPCnspI+rbWjEo9+aN4C++rR4jOhuXpN2PBLG4nr1Pelx60IqcGBrNvIavNG1Z7eT7mNGRO6NRT2INLD9cBmyS2vg56XH6B4s6EFEdKoS/I2o2PYLZMbifV9uUtMYT/bB2DM/7FDrzyICvPDo+ak260t8mB8MOg+1V1pBxYn7QUTEQEakgcU7c9T5mb0i1Z46RER06ooWz0JsoF594PXg15tR13D8TaPfXroXX6zNguw48vLlAxDs52mzfnjqdUgIM68N3l/ATaKJ6MQYyIg0sDgtT51P6BOtdVeIiFyGqb4GD40MVevKlqXn47oP16ColZGybzYcwvTFu9XlZ/7SF2en2v7/4qRIf3W+P7/S5scmItfCQEbUybKKqpCWXaY+lR2fwvVjRES2lBTqifdvGIZAbwPWZhTh0ndXqGmJoqCiFt9uPIRH/rtVXb/9rB64YVQ3+/Qj3BzI8sprUVHTYJfnICLXwKIe5HbaupdMR7RlHxlLMY9h3cIQ5u9ll34QEbmzs3pH4r93jsZNn6zDwcIqXPLOCjU9XAKZxcWDYvHwxGS79cHf24AuwT5q+mRGQSX6xwXb7bmIyLkxkJFbad9eMu3Xln1kFu80B7JzOV2RiMhuZHPn7+86HX//zwZsOFiMitoGSLHDbuH+OLNXBB6blAqdTFWwo6QIfxXIZB0ZAxkRHQ8DGbmV9u4l0x5t2UemtKoeazKK1OVzGMiIiOwqIsAbn986Qk1dDPTxRO/oAPh5dd5bn+4R/li5rxBZxdWqwIisbSMiOhoDGbmltu4lY2u/7s5Do9Gk3hQkNq0vICIi+5GNnsf0itTkuWVaerCvJ0qr65FZVIWeUQGa9IOIHBs/qiHqRIuapitydIyIyPXJhtAybVEcLGK1RSJqHQMZUSepqmvA8vR8dXmCHUosExGR44kP81Xnh4qqte4KETkoBjKiTjJ/S7ZaVC6bhQ6MC9G6O0RE1Am6hviqYiIl1fUor6nXujtE5IAYyIg6yZy1mer8muEJdq/sRUREjrOGLTrQR12W4h5EREdjUQ+iTrDjSCm2ZJXAU++BK4fFad0dIiLqRHGhvsgpq8Ghoir06RKkWT9MJhPmb83G5qwSVWQks7AKtQ2NeHfyUPSJ1a5fRO6OgYzIxtLS0o657V8bStX5abHeyNy9A5k2Oi4RETm++DA/rD9YrEbIJBRJsQ8t/JKWh3u+2HTM7f9cuAuf3jRckz4REQMZUasajEbszatAmJ8XooLMU01OpqzIXLDjuuuua3G7h6cP4u76FDpvP3z3yoP4InPrKfWtoqLilB5PRESdKzbYB3oPD7WOWNaShfp5dXofjEYTXvs5XV0elxyJ8SlRCPbzwoNfbcZvu/PV5tlDE0M7vV9ExEBG1ILRZMKunHKs3l+I8poGddvg+BCM6hEOT/2Jl1xWV5Sp80l/fxzJA4Zab8+o0GFjkQEBBhMue/gZtbi7I9LWLsdPs2eipqamYwcgIiJNGPQ6dAn2waGSamQVVWkSyOZtPYL03HIE+hjwxlWDEeznqW7/Y08+vl5/CDOX7OEoGZFGGMiImmSXVqvpHEWVdeq6t0GH2gYjNmWVYH9BJSakRiEu1O+kxwmPTWyx6fTvqphHLQZ1i0T8KXz6mJu5r8OPJSIibcWF+apAdqi4GgM6udJufaMRry/erS7//czu1jAm7h7XC//deFiNkm3MLMaQBI6SEXU2VlkkkrjU0KgWOksYkyB2Rs8I3HJGEv4yMBYB3gaUVterP1g7j5hHwdoqr6wGeeW1aqpKapdAu/WfiIgcW3zTB3qHmtaRdab/bjiEA4VVCPf3wo2nJ7W4LyHcD5cP6aouz/xlT6f2i4jMGMiIAKw7UIyqukaE+HrixtHd1Dx6mWKSFOGP60YmWMPUkl25arpJW8gf3BX7CtXlHlH+8PPigDQRkbuKDvJRlXar6xtRUGGeidEZauob8eYSc9C6c1xP+Hsf+7dIRsn0Og8s352PTZnFndY3IjLjO0RyeyVVddicWaIuj+kdAW9P/TF7yJyTGo1Gowm7cyswf1s2/jo0DuEB3ic87rbDpaqssPyRG5kUbtevgYiI7FeR1hbHlL8FsSG+OFhYhaziKkQGnvhviK18viYTR0pr1Bq2ySMSWm0jo2SXDe6KuRvMa8k+uZFryYg6EwMZub0/9hag0WRCQpgfksL9W20jJYollEmhj+zSGvyw5Qj+Oiy+1U8aLSHv9z0F6rJMfwz17/wF3ERE7uR4lW5t6VSr3Mq0RQlkMm2xM9ZqSWXFj1ZkqMt3j+8Jn6M+cGzurnE9VSCTUbLCitqTfuhIRLbDQEZuTUaw9uVXqsqHZ/aKOOHeMDKF8aIBsfhqfZZaU/b95sOY1L8LQo6qliVLAxbtzEWD0aQ2Ax0YF9wJXwkRkXs7XqVbW7BVldv4UF91fri4Ws26kFEze5KKwRL+pLLi5UPiTti2W4S/2rR6Z3YZftuTj0sHn7g9EdkOAxm5LfnkUKpKiQFdg9v0aaCvlx4XD4rF3PWH1BqAL9ZmqeqLvaKbCnboDDhQH4js6hp46XU4p0+0ZhuAEhG5o6Mr3dqCrarcyjRFX0+9WkeWU1qDrk0BzV5kxEtcNDD2hKNjFmOTI1UgW7aLgYyoM7GoB7mt3XnlKKysg49Bh5Hd277GS/aPuWZ4vNros67RiB+352DhjhzsQBzi7/sCmQ3mcHZW70gE+fxZWpiIiNybfEAn0+PFgcJKuz5XWU09ftqerS5fObRt4WpcSpQ6l2mLMoJHRJ2DgYzc1tZDpep8UHxImz45bC7Qx1NN/xjWtK9Yek45ihEInZcvPNGI4UlhLHNPRETHSAw3B7KDbazY21ELtmajpt6InlEB6u9cWwyOD0GQj3mrl81ZrLZI1Fk4ZZHcUkmdhyrOIdP3+3Xt2Bovnc4Dp/eMUOvE0nPLUZ29Fys/fgFX3/koBndPsXmfiYjIdQJZfnktKmsbjlsc6lTNXZ9lHR3LyspCQYG50NTJ9I80YEVWA774dRs8+rftg8WIiAgkJLRewZGITo6BjNzS/grz4HCPyIBT/mOYGO6vThuy16I+L0MVCCEiImqN7EkZFeiNvPJaVVgqtUuQzZ9jb14FNmaWqKIhw6OBlNRUVFe1bUTOv+94RFz4IOYs24zpf7u/TY/x9fPDrrQ0hjKiDmIgI7fj4eWLzEpzIOvfwdExIiKiUxklk0Am68jsEci+aSrmMbZ3JExVpSqMTZ72KqITepz0sTWNwILDgHdMT9zx5rfw1Z+84Mmcl6eqETgGMqKOYSAjtyOf/jWaPBDq56mmGxIREXUmmVWx7kAxMgurYJS9UmyoodGIbzeaA9mVw+KAOnNhDwljba0+GVWWqQJjXVAcesXyg0sie2NRD3IrJpMJgYMvUJcHxIWwJD0REXW6LkE+8DLoUNNgRF5ZrU2P/Wt6vgpTYf5eGJ8S3aFjyJ5k4kChfQuPEJEZAxm5lbSCenhFJkLvYUJqDKsgEhFR55OiUPYqf//p6oPq/LLBXVXo64ikcHMgkxE8lr8ncvFA9ttvv+Giiy5CbGysGqn4/vvvjxnNeOqpp9ClSxf4+vpiwoQJ2LNnT4s2RUVFmDx5MoKCghASEoKbb74ZFRUVLdps3boVY8aMgY+PD+Lj4/HKK68c05e5c+ciJSVFtenfvz9+/PFHO33VpKWFe81/+OL9jPBuZ6l7IiIim5e/t+Eo1P78Cvy2O18Vl7p+VGKHjxMVZN7AWvbazC6ttln/iMgBA1llZSUGDhyId955p9X7JTi9+eabmDVrFtasWQN/f39MnDgRNTU11jYSxnbs2IHFixdj/vz5KuTddttt1vvLyspw7rnnIjExERs2bMCrr76KZ555Bv/+97+tbVauXIlrrrlGhblNmzbhkksuUaft27fb+TtAnUlKDK8+bP7Z6R5o1Lo7RETkxhKbRshyympQ22ibY/6naXRsXHKUWqfWUToPD2tg5LRFIhcv6nH++eerU2tkdOyNN97AE088gYsvvljd9umnnyI6OlqNpF199dVIS0vDwoULsW7dOgwbNky1eeutt3DBBRfgtddeUyNvc+bMQV1dHT766CN4eXmhb9++2Lx5M2bMmGENbjNnzsR5552HqVOnquvPP/+8Cnhvv/22CoPkGr5en4UGI1B7ZBdCE7pr3R0iInJjgT6eCPf3QmFlHfJqTv3zcdnTzFJdccrobqd8PJlSuSunHFl23sCaiBx4DVlGRgZycnLUNEWL4OBgjBgxAqtWrVLX5VymKVrCmJD2Op1OjahZ2px55pkqjFnIKFt6ejqKi4utbZo/j6WN5XlaU1tbq0bfmp/Icckc+M/XZKrL5Zs4HZWIiLRnKZ6RVXXqb8e+33wY5TUNSIrwx5ieEad8PMsaNykQUl1noyE8InKuQCZhTMiIWHNy3XKfnEdFRbW432AwICwsrEWb1o7R/DmO18Zyf2teeuklFRAtJ1mbRo5r2a48HC6pRoCXB6p2/aF1d4iIiNCnaQ+y7GoP6APDO3wcmVX06UrzdMXrRyaqoiGnyt/boEbwRFYxR8mI3DKQObpHH30UpaWl1lNWVpbWXaIT+GyN+Q/V+G5+MDXUad0dIiIiVZq+a4jsh+mBgIETO3ycNRlFSM8th5+XHpcPjbNZ/yyjZJmctkjknoEsJiZGnefm5ra4Xa5b7pPzvLy8Fvc3NDSoyovN27R2jObPcbw2lvtb4+3trSo7Nj+RY5Kyvct356vLE3uY/7gQERE5ggFx5o2XAwZMREMHS8x/vCJDnV86uCuCfT3tEshkFI6IXLCox4kkJSWpQLRkyRIMGjRI3SbrtGRt2B133KGujxo1CiUlJap64tChQ9VtS5cuhdFoVGvNLG0ef/xx1NfXw9PT/J+UFOxITk5GaGiotY08z/333299fmkjt5Pzm7P2IOTvyJheEegS6LA/8kRE5IZ6RAbAW2cCAsPx/dp9MLRzuuG2vFr8vKMI8qjTQqqwcePGFvdLAbSO6hrqC72Hh1qbVlJdj1C/P9fjE5HtaPruVPYL27t3b4tCHlIBUdaAJSQkqID0wgsvoFevXiqgPfnkk6pyopSkF6mpqao64q233qqqIUrouvvuu1UFRmknrr32Wjz77LOqpP20adNUKXupqvj6669bn/e+++7DWWedhenTp2PSpEn48ssvsX79+hal8ck51dQ34ut1WdZ59ag9onWXiIiIrPQ6D3TRV+CAMRAfLk/HS3dd1fYH6wzocuOb8IpIQNnGBbj05feO2/ToPVrbwlOvQ5dgHxwqqVazTRjIiFwwkEnoGTdunPX6gw8+qM6nTJmCTz75BA8//LDaq0zK08tI2BlnnKHK3MvmzRZS1l5C2Nlnn62qK15++eVq7zILKbixaNEi3HXXXWoULSIiQm023XyvstGjR+Pzzz9XJfYfe+wxFQCltH6/fv067XtB9jFvyxEUV9WrPyjjU6KwdQsDGREROZaI+lxkmPzhmzQEt73xLQLaOOswvUyH7SUGNcJ20V/Ogdcl5xzTJm3tcvw0e2aLPVzbIz7czxzIiqowMD6kQ8cgIgcOZGPHjj3hnGQPDw8899xz6nQ8MpomYepEBgwYgN9///2Eba688kp1Itcqdf/e8n3q8g2jusGgd9glk0RE5MZ8UI+ajI3w7T4MBV7RSGlD2frymnrsWiUFq0w4MyUG3ZsqNh4tN9P8d/BUNrBeta8Qh4qrYTSabFLBkYha4jtUclk/bc/G/vxKtcD5+lGJWneHiIjouCx7ZO44Uqo2eT6Z33YXqCIgscE+SI0JtFu/IgO94WPQoa7RiJyyjo2yEdGJMZCRS5JP8d5eal6feNPpSQjwZjEPIiJyXNX71sPPox419Ub8sOUI6hqMx227JasEe/Mr4OEBjEuJUjOK7EXn4YF4lr8nsisGMnJJv6TlYldOuQpifxvdTevuEBERnZjJiH7eRfD11COvvBY/bs9WU+9bNDGZ8PuefPzatJXLsMRQRAR4271r3I+MyL4YyMjlyB+st5eZR8duGJWIYD/b7clCRERkL766RvxlYKwqfX+wsApLduWqUCazPmTE7MdtOdiYWaLajuoerk6dwRLIZMpidX1jpzwnkTvhPC5yOb/tKcDWQ6XqU8abz0jSujtERERtFhPsg/P7x2D+lmykZZerU3OyL9g5faKRbMd1Y0cL8vVEZIA38itqsTevAv27mjezJiLb4AgZuRT5FHHmL7vV5ckjEhDeCVM5iIiIbKl7RADOTo06ZpNoPy89Lh3ctVPDmEXv6AB1vju3ZUAkolPHETJyKXPWHFTTOWR07NYzu2vdHSIiog7pGxuM5OhAVUnRwkuv06zsfO/oQKxoKn9fUdvAYllENsQRMnIZWUVVeOmnXeryI+enIDrozw3EiYiInI3sn+njqbeetNwDTKYtdgk2/13dw1EyIptiICOXmao49ZstqKprxPCkMFw/kvuOERER2XqUTOzOrdC6K0QuhYGMXMKctZlYvd9cLvjVKwZo+ikiERGRK+oVFQCPpmqLpdX1WneHyGUwkJHTO1BQiZd+TFOXp52XjMRwf627RERE5HL8vQ2IC/VVl1ncg8h2GMjIqWUUVOLa91ebpyp2C8MNo7gJNBERkb2nLaYzkBHZDAMZOS1ZVPzXf63CkdIa9Ij0x1vXDuZURSIiIjvqGRUA+VNbWFGHwoparbtD5BIYyMgp7ThSiqv+vRr55bVIiQnEV38fxaqKREREdibVHi1LA7YdLtW6O0QugZtIkFMprarHu7/uxccrD6CuwYgBccH49KbhCPHz0rprREREbmFgXLBaMrD1UCnCojkzhehUMZCR0wSxr9Zn4p1l+6yVncb0isA7k4cgyMdT6+4RERG5DRkhS44JRHpOOTYU6QGdoU3b0/xvy2F8+EcGKmoa1B5rBp0HYoJ9cM/4XhiaGNopfSdyRAxk5JBMJpPa5+TX9Dws2ZWHDQeL0Wg0qft6Rwdg2nkpGJ8SBQ8PfjJHRETU2c7qFYnMwiqU1QPBI684Yds1+wvx4o9pakTtaLtyyvFrej4uG9wVj5yfgiguPyA3xEBGDiO7tBqLduRi9f5CrM0oQmFlXYv7k6MDcfOYJFw+JA56Fu8gIiLSjK+XHmf1jsTCHTkIHnUVDpXVY8hRbTKatqVZtDNXXQ/wNuCOsT0wIikM9Y0mNBiNmL8lG19vyMK3mw7j5x05ePDcZNw4uhuLdJFbYSAjTdXUN2LxzlzM3XAIf+zJR9MgmOLjqcPwpHCcnRKlRsPiw/y07CoRERE1IzNWNu8zIqfGE6+uLEGRZwaGdQtT0xDfXroXn60+iAajSVVlvGZ4Ah44pzciArxbHGNMr0hcOyIBT/+wA5uzSvD8/J1YtCMHr105kH/3yW0wkJFmUxK/33wYLy5IQ0HFnyNhp3ULxdjkKPXp2YC4EHgZWAiUiIjIEcmygcFhDViQUY+sMn88M2/nMW3GJUfisQtS0atp/7LWDIwPwbd3jMbnazPxjx/TsCajCOe98Rsem5SKvw6Lh6ee7wXItTGQUaeTKQxPfL8NK/YWqutdgn1wxdA4dbKU0iUiIiLH52cAsj+5D1Pf+hKH63yxMbMY5TUNakuaJyb1wRm9Itp0HJmieN3IRFWw6//mbsG6A8V4/LvtmLFoNy4Z3FW9R0jtEmT3r4dICwxk1KmjYrNXHsA/ftqlStZ7G3S49+xeuHVM9xYjYZmZmSgoKLBLH9LS0uxyXCIiInfVUJKDvh6HcMWgVDQOiEBJjRGhvjroyjOxcWNmu4/38DBvLAgOxPfplWo9uVRmlFO4vxe6hPggJsgXcaG+GBQfoqozymUW+SJnxkBGnaKh0Yhn5+3Ef1YfVNflE7AXLul3zIiYhLGU1FRUV1XZtT8VFRV2PT4REZE7KCvKV+fXXXed7Q+u0yMoeSTG3/QothVBhTM5bT9c1qJZqI8OI+N8cHXfQAR6t296Y0REBBISEmzccaL2YSAjuyuvqcfdn2/C8t35kA+wHj0/RY2KtfZployMSRibPO1VRCf0sHlf0tYux0+zZ6KmpsbmxyYiInI31RXmcDTp748jecBQmx57//b1+P69f+D7qRfCw8sXhpAYGAIjoA+MgGdYV3h3TYVXdA8U1xjw094qzN9yBMVLP0Dlzl/b/By+fn7YlZbGUEaaYiAju8oprcGUj9YiPbccvp56vHH1IEzsG3PSx0kYi+vV1+b9yc3cZ/NjEhERubvw2ESb/922/M0+UdiT0vn5tfXYVmxAuX8IIi76P/S58kEMDWtQ69tOdvw5L09VHwYzkJGWGMjIbg4WVmLyB2twqLgakYHe+GjKaegfF6x1t4iIiMiFwl43AEOMJmzILFb7mObV6PB7oR8uHdwVYf5endpXoo5gHVGyi/Scclwxa5UKY4nhfqqcLcMYERER2YNe54Hh3cJw3YgEhPp5oqK2AXPXZyG7tFrrrhGdFEfIqENOVAlxd2EdXvi9CBV1JiQGG/DU6ADkH9iF/AMnPy6rIBIREVFHhfh54cqh8fhhyxHklNXg242HcUH/LkiK4LY65LgYyKjdTlQJ0bf3KERc+BB0nj6oPbwLf8x8BhNq2l/RkFUQiYiIqCN8vfS4bEhXLNiWjYOFVZi39QjG9o7EgLgQrbtG1CoGMmq31iohmkzA7nIdtpeYf6SifYwYOaI7DKM+bdexWQWRiIiITpWnXoeLBsRiSVou0nLKsSw9HyVV9Wqjah33LCMHw0BGHWaphCh7jMl/dDtLzKVvB8YF48xekdDp2v8fHqsgEhERka3WlZ3TJ1pNY1y1vxCbskpQUl2PiX2j4W3Qa909IisGMjrlsvaLduaguKoeEr/O7B2JQfGcEkBERETakz1PhyeFIcTPE4t25iKjoBKzVx7EyO5hCDFp3TsiMwYy6hi9J7YV67EnMwvy/5mflx7n9olGYjgXzRIREZFj6R0diEAfAxbtyFWjZDKzJ8DgCb+UMahpMGrdPXJzDGTULjX1jVi4txJdb52F3eXm4f6UmECc1TsSPp4c/iciIiLH1CXYF9eNTMT2w6VYk1GEivpGRF48DVO+z8Xo7WtxdmoURvcIR4/IADWyRtRZGMioTYor6/DfjYfw79/2I6+8FobgaPjoTTinbyy6RwZo3T0iIiKiNq0rGxgfgpQugfh10x5sy8wHQmKwfHe+OgnZTHpYYihGdg9XIY2zf8jeGMjouCprG/BLWi5+2HxE/SfVYDRPto7w0yH9+3dw8003I5FhjIiIiJyMFPXoG9KInx+/Bd8vXY1sXQSW7crD5qwSFFXWqfVmcnpu/k70igrA2anROKdPFAbFh6pQR2RLDGRkZTKZsL+gEr+m5+PX9Dys2V+EusY/51X36RKEG0YlIskjHyOfXgC97mZN+0tERER0quKDPXHxkB64/aweqG1oVFMa12YU4/c9+Wpq4568CnWatXwfwv29MC4lCuOSozA4IQRdgn04vZFOGQOZC2/eLPuFnUxtgwnb82uxMdt8yq1sbHF/TIAeYxJ8MSbBB3FBnrILGdLSdtmx50RERESdJy0trcV1iVcjgoARQ71Q2T8Km3Jqse5IDTZk16Kwsg7fbDikTiLYW4ceoZ6I9NfDz9MD/p46+Bg8zAXP/PwRHBKi9kTz9dTD21OHAG8D4sP8kBDmx7X3ZMVA5qJhLCU1VW3e3BpDSBf4dh8K3x7D4B3fHzpPb+t9poZ61Bzajup961G9fwMOFh3CGgCvtXKciooKO34VRERERPZTVmReM3bddde17QE6Pbzj+sCv5wj4JAyAZ2QiSmuBjTm1x3sGANnHPVyYrw4JwZ5ICfdE73Av9A73hJ+nrk1diYiIQEJCQtv6TQ6PgcwFyciYhLHJ015Vmzc3moD8Gg/k1uiQU61DRUPLoXU/vQnRvkbE+BgR5WOCoXtf4My+AKa0evy0tcvx0+yZqKmp6aSviIiIiMi2qiskMAGT/v44kgcMbffjG42NKK03orjOA7WNHqg3AfVGoMEEVJYW4/Ce7VBDZXo9PAze6gNwD29/eIbEQOcTgKJqI4qqa7G5KdCZjI2oy8tAbdZ21GRuRW3WDhhrK1t9bl8/P+xKS2MocxEMZC5KqiBWhPbCocogHCquthbkELIWNTbEF93C/dEt3E9VE2rP/OfczH126jURERFR5wqPTURcL/kg2nY2LPkBW+a91mrYM5mAOmOd+oC8pM4DhbVy0qEKenjH9FSnoNMukZYI9jQh0seECG8jQr1M8NUDeVn7MOflqeoDeAYy18BAdpR33nkHr776KnJycjBw4EC89dZbGD58OJzJi78XoevtH2JzsVwzT1uUOcuJ4X4qhMWH+arqQkRERETkGGGvorYBh4urcaikSp0XV9WjtN4DpfXA3qa9X30MOgR6JyP07Nvwv/QK5Hhmo2uIr/pwPdjPEwFeBuhYBdLpMJA189VXX+HBBx/ErFmzMGLECLzxxhuYOHEi0tPTERUVBWfRJcAAk7EKkb4eSI6LVPtnRAS0bxSMiIiIiDqPfHieHBOoTpbthw6XVKuZTkdKq1U5/poGI2oadAga9hfM3lKO2Vs2tjiGZDEpKmLQeUDvYd53zaBDs+tStMT8flDeFlreGTa/LLw8PeHj421u23SfuU2zxzZdl3N1fJ1OnRv05uc36Jtf16m+yKm8rAzV1dXW44ij36H+efuf95y8rVlQgD8eu3QYnAkDWTMzZszArbfeihtvvFFdl2C2YMECfPTRR3jkkUfgLC5L9ce7d12IK2b8B3HdwrTuDhERERG1k7+3Ab2jA9VJNDQaVSjbsnUrVi1fAkNwFAxBUdAHRULvGwgPgxdkhUqVLGZTi9dORb11lpWzMTUU4bqhUU41nZOBrEldXR02bNiARx991HqbTqfDhAkTsGrVqmPa19bWqpNFaWmpOi8rMy8Q1ZKhoRqm2koc2rMDtdW2/2WyrCHLObAb+/z9nObY9j4++67N8dl3bY7PvmtzfPZdm+Oz79ocn31vXf2eFShZ/glOO/8qxAV5ASgBqgEjPNAAPRqhh8lDSu+by++bIJUbLZfN40hHx7U/r3ugOP8I0jesaBodsww/qTGyZpfNbdXsK3XSwUNvUJc9dHpAb4CHh15VplTXdTrzuYcM0Xkgtkcf+AUGH+cr9Djp96B5f5urranE4T07ceBAOEJCQqAlSyaQfX5PxsPUllZu4MiRI+jatStWrlyJUaNGWW9/+OGHsXz5cqxZI8Xf//TMM8/g2Wef1aCnRERERETkDLKyshAXF3fCNhwh6yAZSZP1ZhZGoxFFRUUIDw/XfK2WJPL4+Hj1AxAUFKRpX8iMr4nj4WviWPh6OB6+Jo6Hr4lj4evheMoc6DWRMa/y8nLExsaetC0DWbMN9vR6PXJzc1vcLtdjYmKOae/t7a1OzWk9NHo0+UHU+oeRWuJr4nj4mjgWvh6Oh6+J4+Fr4lj4ejieIAd5TYKDjzcts6W2bQfuBry8vDB06FAsWbKkxaiXXG8+hZGIiIiIiMhWOELWjExBnDJlCoYNG6b2HpOy95WVldaqi0RERERERLbEQNbMVVddhfz8fDz11FNqY+hBgwZh4cKFiI6OhjORqZRPP/30MVMqSTt8TRwPXxPHwtfD8fA1cTx8TRwLXw/H4+2krwmrLBIREREREWmEa8iIiIiIiIg0wkBGRERERESkEQYyIiIiIiIijTCQERERERERaYSBzIn99ttvuOiii9QO4B4eHvj+++9b3C/1WqRiZJcuXeDr64sJEyZgz549mvXX1b300ks47bTTEBgYiKioKFxyySVIT09v0aampgZ33XUXwsPDERAQgMsvv/yYzcjJdt577z0MGDDAukGk7Cn4008/We/n66Gtf/7zn+r/rvvvv996G1+TzvXMM8+o16D5KSUlxXo/Xw9tHD58GNddd536vsvf7/79+2P9+vXW+/n3vXN169btmN8TOcnvhuDvSedqbGzEk08+iaSkJPXz36NHDzz//PPq98JZf0cYyJyY7JE2cOBAvPPOO63e/8orr+DNN9/ErFmzsGbNGvj7+2PixInqPw6yveXLl6v/kFevXo3Fixejvr4e5557rnqdLB544AHMmzcPc+fOVe2PHDmCyy67TNN+u7K4uDj1pn/Dhg3qzcz48eNx8cUXY8eOHep+vh7aWbduHf71r3+pwNwcX5PO17dvX2RnZ1tPf/zxh/U+vh6dr7i4GKeffjo8PT3VB0g7d+7E9OnTERoaam3Dv++d//9V898R+RsvrrzySnXO35PO9fLLL6sPXN9++22kpaWp6/I78dZbbznv74iUvSfnJy/ld999Z71uNBpNMTExpldffdV6W0lJicnb29v0xRdfaNRL95KXl6del+XLl1u//56enqa5c+da26Slpak2q1at0rCn7iU0NNT0wQcf8PXQUHl5ualXr16mxYsXm8466yzTfffdp27na9L5nn76adPAgQNbvY+vhzamTZtmOuOMM457P/++a0/+z+rRo4d6Lfh70vkmTZpkuummm1rcdtlll5kmT57stL8jHCFzURkZGWpzaxmitQgODsaIESOwatUqTfvmLkpLS9V5WFiYOpdRGhk1a/6ayNSghIQEviadNMXhyy+/VCOWMnWRr4d2ZCR50qRJLb73gq+JNmQaj0x97969OyZPnozMzEx1O18Pbfzwww8YNmyYGn2R6e+DBw/G+++/b72ff9+1VVdXh88++ww33XSTmrbI35PON3r0aCxZsgS7d+9W17ds2aJG9s8//3yn/R0xaN0Bsg/5QRTR0dEtbpfrlvvIfoxGo1oXI9NO+vXrp26T77uXlxdCQkJatOVrYl/btm1TAUymKcjc/u+++w59+vTB5s2b+XpoQELxxo0b1RSgo/F3pPPJG5RPPvkEycnJairWs88+izFjxmD79u18PTSyf/9+NR3rwQcfxGOPPaZ+V+699171WkyZMoV/3zUm6/VLSkrwt7/9TV3n70nne+SRR1BWVqaCr16vVx+4vvjii+oDJeGMvyMMZER2GgGQNzTN12KQNuSNpoQvGbH85ptv1BsameNPnS8rKwv33XefWn/h4+OjdXcIsH6iLGQ9nwS0xMREfP3112ohPGnzgZ6MkP3jH/9Q12WETP6eyFoY+f+LtPXhhx+q3xsZVSZtfP3115gzZw4+//xztQZW/sbLh+Dymjjr7winLLqomJgYdX50lR+5brmP7OPuu+/G/PnzsWzZMlVUwkK+7zLVQT5Za46viX3JJ5c9e/bE0KFDVSVMKYQzc+ZMvh4akKk9eXl5GDJkCAwGgzpJOJaF13JZPr3ka6It+ZS/d+/e2Lt3L39HNCJV4WQUv7nU1FTrVFL+fdfOwYMH8csvv+CWW26x3sbfk843depUNUp29dVXqwqk119/vSqsIn/jnfV3hIHMRUkpUPmhkzm2FjK8K5VmZPoW2Z7UVpEwJlPili5dql6D5iQQSNWs5q+JlMWXP7J8TTr30+fa2lq+Hho4++yz1RRS+TTTcpKRAJlmYrnM10RbFRUV2LdvnwoF/B3Rhkx1P3rLFFkrIyOXgn/ftfPxxx+rdX2yBtaCvyedr6qqCjpdywgjUxfl77vT/o5oXVWETq1S2aZNm9RJXsoZM2aoywcPHlT3//Of/zSFhISY/ve//5m2bt1quvjii01JSUmm6upqrbvuku644w5TcHCw6ddffzVlZ2dbT1VVVdY2t99+uykhIcG0dOlS0/r1602jRo1SJ7KPRx55RFW5zMjIUL8Dct3Dw8O0aNEidT9fD+01r7Io+Jp0roceekj9nyW/IytWrDBNmDDBFBERoarECr4enW/t2rUmg8FgevHFF0179uwxzZkzx+Tn52f67LPPrG34973zNTY2qt8FqYJ5NP6edK4pU6aYunbtapo/f776v+vbb79V/289/PDDTvs7wkDmxJYtW6aC2NEn+UG1lP38//buLrTHNo4D+G9ZbFGUpClbiexAiiIlITLlhMhSjlAikRyYxo44kjN5q3k7kZckoqSUIoUDm1LzkgOOHNjycqDk1nXX1vN/Hi8rnl3bfD71X9277n9d/93du/ftun6/7du3r5g4cWLZ6nPJkiVFV1dX7mkPW9+7Ful16tSpvnPSL4KtW7eWrdfTA3bVqlVlaOP/kdriNjQ0FCNHjiwmTJhQ3gO9YSxxPQZfIHNNBlZzc3NRV1dX3iPpD5x0/OLFi75x1yOPa9euFTNmzCif3Y2NjcWJEycqxj3fB97NmzfLZ/r3fs7uk4H1/v378rmRQnBNTU0xZcqUorW1tfj8+fOQvUeq0pfcq3QAAAB/IzVkAAAAmQhkAAAAmQhkAAAAmQhkAAAAmQhkAAAAmQhkAAAAmQhkAAAAmQhkAAAAmQhkAAAAmQhkANAP9+/fjxEjRsSKFStyTwWAYaSqKIoi9yQAYLDbtGlTjBkzJtrb26OrqysmTZqUe0oADANWyADgFz5+/Bjnz5+PLVu2lCtkp0+frhi/evVqTJs2LWpqamLx4sVx5syZqKqqip6enr5z7t69GwsWLIja2tqYPHlybN++PT59+pTh0wAwmAhkAPALFy5ciMbGxpg+fXqsX78+Tp48Gb0bTF69ehVr1qyJlStXRkdHR2zevDlaW1sr3v/y5ctYvnx5rF69Ojo7O8twlwLatm3bMn0iAAYLWxYB4Bfmz58fa9eujR07dsSXL1+irq4uLl68GIsWLYqWlpa4fv16PHnypO/8vXv3xoEDB6K7uzvGjRtXbndM9WfHjx/vOycFsoULF5arZGllDYC/kxUyAPiJVC/24MGDWLduXXlcXV0dzc3NZS1Z7/icOXMq3jN37tyK47RylrY5phq03ldTU1N8/fq1XGED4O9VnXsCADCYpeCVVsX+2cQjbS4ZNWpUHD58uN81aGkrY6ob+7f6+vo/Ol8AhhaBDAB+IAWxs2fPxqFDh2LZsmUVY6lm7Ny5c2Vd2Y0bNyrGHj58WHE8e/bsePr0aUydOnVA5g3A0KGGDAB+4MqVK+X2xLdv38bYsWMrxnbv3h23b98uG36kULZz587YuHFjPH78OHbt2hVv3rwpuyym96VGHvPmzYsNGzaU9WSjR48uA9qtW7f6vcoGwPCkhgwAfrJdcenSpf8JY0nqmPjo0aP48OFDXLp0KS5fvhwzZ86Mo0eP9nVZTNsak/T9O3fuxLNnz8rW97NmzYq2tjb/ywwAK2QA8KelDovHjh2L169f554KAIOcGjIA+E1HjhwpOy2OHz8+7t27FwcPHvQ/xgDoF4EMAH7T8+fPY//+/fHu3buya2KqIduzZ0/uaQEwBNiyCAAAkImmHgAAAJkIZAAAAJkIZAAAAJkIZAAAAJkIZAAAAJkIZAAAAJkIZAAAAJkIZAAAAJHHNzDmck8S7wiKAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# prompt: draw the plot for the users_filtered\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming users_filtered is a Pandas DataFrame (convert if necessary)\n",
        "# If users_filtered is a Spark DataFrame, convert it to Pandas:\n",
        "filtered_users = filtered_users.toPandas()\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(filtered_users['Age'], bins=30, kde=True)  # Adjust bins as needed\n",
        "plt.title('Distribution of User Ages')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Number of Users')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i956bGeRSqSw"
      },
      "source": [
        "# Làm việc với 3 bảng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSoj0sxntTcf",
        "outputId": "ca0c6b45-bec5-429d-abb9-e1d3fc716eb5"
      },
      "outputs": [],
      "source": [
        "users_rating_df = filtered_users"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc5788O_TPbX"
      },
      "source": [
        "##Book table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGLV_BUaCpHo",
        "outputId": "aede30d5-c276-4aaf-f1a6-1d370a03f064"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'books_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of Books DataFrame before potential duplicate removal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbooks_df\u001b[49m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m books_df\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISBN\u001b[39m\u001b[38;5;124m'\u001b[39m, keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of Books DataFrame after duplicate ISBN removal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbooks_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'books_df' is not defined"
          ]
        }
      ],
      "source": [
        "print(f\"Shape of Books DataFrame before potential duplicate removal: {books_df.shape}\")\n",
        "books_df.drop_duplicates(subset='ISBN', keep='first', inplace=True)\n",
        "print(f\"Shape of Books DataFrame after duplicate ISBN removal: {books_df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 895
        },
        "collapsed": true,
        "id": "REOsOd_sCr7d",
        "outputId": "acb2fcac-b4cb-4098-eb72-2fb50b0bde80"
      },
      "outputs": [],
      "source": [
        "print(books_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TVAk2nYTVR2"
      },
      "source": [
        "##Users_rating table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "collapsed": true,
        "id": "bY4WkKnz3IzO",
        "outputId": "387d0720-9cfd-4ec8-eff9-0c041c08044d"
      },
      "outputs": [],
      "source": [
        "users_rating_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create total data dataframe\n",
        "total_data_df = pd.merge(users_rating_df, books_df, on='ISBN', how='left', validate='m:m')\n",
        "\n",
        "# Count ratings per book\n",
        "book_ratings_count = total_data_df.groupby('ISBN')['Book-Rating'].count()\n",
        "\n",
        "# Get books with less than 50 ratings\n",
        "books_to_drop = book_ratings_count[book_ratings_count < 50].index\n",
        "\n",
        "# Drop books with less than 50 ratings\n",
        "print(f\"\\nShape before dropping unpopular books: {total_data_df.shape}\")\n",
        "total_data_df = total_data_df[~total_data_df['ISBN'].isin(books_to_drop)]\n",
        "print(f\"Shape after dropping unpopular books: {total_data_df.shape}\")\n",
        "\n",
        "# Check for NaN values in total_data_df\n",
        "print(\"\\nNaN values in total_data_df:\")\n",
        "print(total_data_df.isnull().sum())\n",
        "# Show rows with NaN values\n",
        "print(\"\\nRows with NaN values:\")\n",
        "print(total_data_df[total_data_df.isnull().any(axis=1)].head())\n",
        "# Drop NaN values from total_data_df\n",
        "filtered_data_df = total_data_df.dropna()\n",
        "print(\"\\nShape after dropping NaN values:\", total_data_df.shape)\n",
        "\n",
        "print(f\"Shape after filtering: {filtered_data_df.shape}\")\n",
        "filtered_data_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO_rAM6WVc-p"
      },
      "source": [
        "# Colaborative filtering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnK122SGVm16"
      },
      "source": [
        "## Matrix Factorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JJZKWJZUfkb6",
        "outputId": "02b2e5d1-970a-4352-e62d-7fdb383adb8c"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "ratings_spark_df = spark.createDataFrame(total_data_df)\n",
        "ratings_spark_df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IughzAEiTZrR"
      },
      "outputs": [],
      "source": [
        "\n",
        "isbn_indexer = StringIndexer(inputCol=\"ISBN\", outputCol=\"bookId\")\n",
        "ratings_indexed = isbn_indexer.fit(ratings_spark_df).transform(ratings_spark_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oysz_F7bTbkU"
      },
      "outputs": [],
      "source": [
        "(training, test) = ratings_indexed.randomSplit([0.8, 0.2], seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# 1. Create the ALS model\n",
        "als = ALS(\n",
        "    maxIter=10,\n",
        "    regParam=0.01,\n",
        "    rank=10,  # Increased rank for more features\n",
        "    userCol=\"User-ID\",\n",
        "    itemCol=\"bookId\",\n",
        "    ratingCol=\"implicit_rating\",\n",
        "    coldStartStrategy=\"drop\",\n",
        "    nonnegative=True,  # Ensures non-negative predictions\n",
        "    implicitPrefs=True\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Define parameter grid for tuning\n",
        "param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(als.rank, [15]) \\\n",
        "    .addGrid(als.regParam, [0.05]) \\\n",
        "    .addGrid(als.maxIter, [15]) \\\n",
        "    .build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Define evaluator\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName=\"rmse\",\n",
        "    labelCol=\"implicit_rating\",\n",
        "    predictionCol=\"prediction\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Create CrossValidator\n",
        "cv = CrossValidator(\n",
        "    estimator=als,\n",
        "    estimatorParamMaps=param_grid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Cache the datasets\n",
        "training.cache()\n",
        "test.cache()\n",
        "\n",
        "# 7. Fit the model using cross-validation\n",
        "print(\"Training model with cross-validation...\")\n",
        "cvModel = cv.fit(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. Get the best model\n",
        "bestModel = cvModel.bestModel\n",
        "\n",
        "# 9. Print the best parameters\n",
        "# Print the best parameters\n",
        "print(\"\\nBest Model Parameters:\")\n",
        "print(f\"Rank: {bestModel.rank}\")\n",
        "print(\"\\nAll Model Parameters:\")\n",
        "print(bestModel.explainParams())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10. Make predictions on validation set\n",
        "predictions = bestModel.transform(test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 11. Evaluate final model\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"\\nRoot-mean-square error on validation set = {rmse}\")\n",
        "\n",
        "# 12. Show sample predictions\n",
        "print(\"\\nSample Predictions:\")\n",
        "predictions.select(\"User-ID\", \"ISBN\", \"implicit_rating\", \"prediction\").show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add these debug prints after your predictions\n",
        "print(\"Number of rows in predictions:\", predictions.count())\n",
        "print(\"Sample of predictions:\")\n",
        "predictions.show(5)\n",
        "\n",
        "# Check for any null values\n",
        "print(\"Number of null values in prediction column:\", predictions.filter(col(\"prediction\").isNull()).count())\n",
        "print(\"Number of null values in Age column:\", predictions.filter(col(\"Age\").isNull()).count())\n",
        "\n",
        "# Check the data types\n",
        "print(\"Schema of predictions DataFrame:\")\n",
        "predictions.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60Pe_-GvV3I0",
        "outputId": "047d49ef-8f51-418e-d272-676e44fa280d"
      },
      "outputs": [],
      "source": [
        "# Recommend top 10 books for each user\n",
        "user_recs = bestModel.recommendForAllUsers(10)\n",
        "user_recs.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIwa--uTeURL",
        "outputId": "6d10cebd-7748-4eb7-b8d1-74ed18bf233f"
      },
      "outputs": [],
      "source": [
        "# Assuming you have `books` and `ratings_indexed` Spark DataFrames from the preceding code\n",
        "# You also need the `isbn_indexer` fitted from the previous step to get the original ISBNs\n",
        "\n",
        "# Get the mapping from indexed bookId back to original ISBN\n",
        "# This can be done by accessing the labels from the fitted StringIndexer model\n",
        "# You need to refit the indexer to get the labels again if the previous fit was not saved\n",
        "fitted_isbn_indexer = isbn_indexer.fit(ratings_spark_df)\n",
        "book_id_to_isbn = fitted_isbn_indexer.labels\n",
        "\n",
        "# Create a Spark DataFrame for the book ID to ISBN mapping\n",
        "# The index of the label corresponds to the indexed bookId\n",
        "# Ensure the data type matches the bookId in the recommendations struct (IntegerType)\n",
        "from pyspark.sql.types import IntegerType\n",
        "book_id_mapping = spark.createDataFrame([(i, isbn) for i, isbn in enumerate(book_id_to_isbn)], [\"bookId\", \"Original_ISBN\"])\n",
        "book_id_mapping = book_id_mapping.withColumn(\"bookId\", col(\"bookId\").cast(IntegerType())) # Cast to IntegerType\n",
        "\n",
        "# Now, flatten the recommendations list to join with the main books DataFrame\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "user_recs_exploded = user_recs.select(\"User-ID\", explode(\"recommendations\").alias(\"recommendation\"))\n",
        "\n",
        "# Join with the book ID to ISBN mapping\n",
        "# Access the bookId from the exploded 'recommendation' struct\n",
        "user_recs_with_isbn = user_recs_exploded.join(book_id_mapping, user_recs_exploded.recommendation.getItem(\"bookId\") == book_id_mapping.bookId, \"inner\")\n",
        "\n",
        "# Join with the original books DataFrame to get book details\n",
        "# Ensure the 'ISBN' column in the 'books' DataFrame is consistent\n",
        "# Assuming 'ISBN' is the correct column name in your original books Spark DataFrame\n",
        "book_details_recs = user_recs_with_isbn.join(books, user_recs_with_isbn.Original_ISBN == books.ISBN, \"inner\")\n",
        "\n",
        "# Select relevant columns\n",
        "final_recommendations = book_details_recs.select(\n",
        "    user_recs_with_isbn[\"User-ID\"],\n",
        "    user_recs_with_isbn.Original_ISBN.alias(\"Recommended_ISBN\"),\n",
        "    books[\"Book-Title\"],\n",
        "    books[\"Book-Author\"],\n",
        "    user_recs_with_isbn.recommendation.getItem(\"rating\").alias(\"Estimated_Rating\")\n",
        ")\n",
        "\n",
        "# Show the final recommendations with book details\n",
        "final_recommendations.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate RMSE, Precision@10, Recall@10 based on implicit ratings and predicted ratings\n",
        "from pyspark.sql.functions import col, explode\n",
        "from math import sqrt\n",
        "\n",
        "# Get predictions for all user-item pairs in test data\n",
        "predictions = bestModel.transform(test)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = sqrt(predictions.select(\n",
        "    ((col(\"implicit_rating\") - col(\"prediction\"))**2).alias(\"squared_error\")\n",
        ").agg({\"squared_error\": \"avg\"}).collect()[0][0])\n",
        "\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "\n",
        "# For Precision and Recall, we need to:\n",
        "# 1. Get top 10 predictions for each user\n",
        "# 2. Compare with actual implicit ratings\n",
        "\n",
        "# Get top 10 predictions per user\n",
        "top_k_predictions = bestModel.recommendForAllUsers(10)\n",
        "top_k_predictions = top_k_predictions.select(\n",
        "    col(\"User-ID\"),\n",
        "    explode(\"recommendations\").alias(\"recommendation\")\n",
        ").select(\n",
        "    col(\"User-ID\"),\n",
        "    col(\"recommendation.bookId\").alias(\"bookId\")\n",
        ")\n",
        "\n",
        "# Get actual positive ratings (implicit_rating = 1)\n",
        "actual_positives = test.filter(col(\"implicit_rating\") == 1)\n",
        "actual_positives = actual_positives.select(\"User-ID\", \"bookId\")\n",
        "\n",
        "# Calculate precision and recall per user\n",
        "user_metrics = top_k_predictions.join(\n",
        "    actual_positives.groupBy(\"User-ID\").agg(collect_list(\"bookId\").alias(\"actual_positives\")), \n",
        "    \"User-ID\"\n",
        ")\n",
        "\n",
        "def calculate_precision_recall(row):\n",
        "    predicted = set([row.bookId])\n",
        "    actual = set(row.actual_positives)\n",
        "    \n",
        "    true_positives = len(predicted.intersection(actual))\n",
        "    precision = true_positives / len(predicted) if len(predicted) > 0 else 0\n",
        "    recall = true_positives / len(actual) if len(actual) > 0 else 0\n",
        "    \n",
        "    return (precision, recall)\n",
        "\n",
        "metrics = user_metrics.rdd.map(calculate_precision_recall)\n",
        "avg_precision, avg_recall = metrics.mean()\n",
        "\n",
        "\n",
        "print(f\"Precision@10: {avg_precision:.4f}\")\n",
        "print(f\"Recall@10: {avg_recall:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create mapping between numeric IDs and ISBNs\n",
        "# Assuming you have the original mapping from when you created the model\n",
        "fitted_isbn_indexer = isbn_indexer.fit(ratings_spark_df)\n",
        "book_id_to_isbn = fitted_isbn_indexer.labels\n",
        "\n",
        "# Create a reverse mapping (ISBN to numeric ID)\n",
        "isbn_to_id = {isbn: idx for idx, isbn in enumerate(book_id_to_isbn)}\n",
        "\n",
        "def get_reranked_recommendations(user_id, current_book_isbn, num_recommendations=10):\n",
        "    # Convert ISBN to numeric ID\n",
        "    current_book_id = isbn_to_id.get(current_book_isbn)\n",
        "    if current_book_id is None:\n",
        "        print(f\"Book ISBN {current_book_isbn} not found in the mapping\")\n",
        "        return None\n",
        "    print(f\"Current book ID: {current_book_id}\")\n",
        "    \n",
        "    # Step 1: Get initial ALS recommendations\n",
        "    try:\n",
        "        user_recs = bestModel.recommendForUserSubset(\n",
        "            spark.createDataFrame([(user_id,)], [\"User-ID\"]), \n",
        "            num_recommendations\n",
        "        )\n",
        "        user_recs = user_recs.collect()\n",
        "        \n",
        "        if not user_recs:\n",
        "            print(f\"No recommendations found for user {user_id}\")\n",
        "            return None\n",
        "        \n",
        "        # Get recommendations list\n",
        "        recommendations = user_recs[0].recommendations\n",
        "        \n",
        "        # Step 2: Get the current book's features\n",
        "        current_book_features = None\n",
        "        for item in bestModel.itemFactors.collect():\n",
        "            if item.id == current_book_id:\n",
        "                current_book_features = np.array(item.features)\n",
        "                break\n",
        "        if current_book_features is None:\n",
        "            print(f\"Current book ID {current_book_id} not found in the model\")\n",
        "            return None\n",
        "        \n",
        "        # Step 3: Calculate similarities and create a list of (ISBN, rating, similarity)\n",
        "        recommendations_with_similarity = []\n",
        "        for rec in recommendations:  # Use recommendations instead of user_recs[0].recommendations\n",
        "            rec_features = None\n",
        "            for item in bestModel.itemFactors.collect():\n",
        "                if item.id == rec.bookId:\n",
        "                    rec_features = np.array(item.features)\n",
        "                    break\n",
        "            \n",
        "            if rec_features is not None:\n",
        "                # Convert current book features to numpy array if not already\n",
        "                if not isinstance(current_book_features, np.ndarray):\n",
        "                    current_book_features = np.array(current_book_features)\n",
        "                \n",
        "                # Calculate cosine similarity using numpy\n",
        "                dot_product = np.dot(current_book_features, rec_features)\n",
        "                norm_current = np.linalg.norm(current_book_features)\n",
        "                norm_rec = np.linalg.norm(rec_features)\n",
        "                similarity = float(dot_product / (norm_current * norm_rec))\n",
        "                \n",
        "                # Convert numeric ID back to ISBN for display\n",
        "                isbn = book_id_to_isbn[rec.bookId]\n",
        "                recommendations_with_similarity.append((isbn, rec.rating, similarity))\n",
        "            else:\n",
        "                print(f\"Could not find features for bookId: {rec.bookId}\")\n",
        "        \n",
        "        if not recommendations_with_similarity:\n",
        "            print(\"No similar books found\")\n",
        "            return None\n",
        "        \n",
        "        # Step 4: Create DataFrames for recommendations and similarities\n",
        "        recommendations_df = spark.createDataFrame(\n",
        "            [(isbn, rating) for isbn, rating, _ in recommendations_with_similarity],\n",
        "            [\"ISBN\", \"rating\"]\n",
        "        )\n",
        "        \n",
        "        similarities_df = spark.createDataFrame(\n",
        "            [(isbn, similarity) for isbn, _, similarity in recommendations_with_similarity],\n",
        "            [\"ISBN\", \"Similarity\"]\n",
        "        )\n",
        "        \n",
        "        # Step 5: Join with book details and similarities\n",
        "        book_details_recs = recommendations_df.join(\n",
        "            books,\n",
        "            recommendations_df.ISBN == books.ISBN,\n",
        "            \"inner\"\n",
        "        ).join(\n",
        "            similarities_df,\n",
        "            recommendations_df.ISBN == similarities_df.ISBN,\n",
        "            \"inner\"\n",
        "        )\n",
        "        \n",
        "        # Step 6: Calculate final score (combine ALS rating and similarity)\n",
        "        als_weight = 0.7\n",
        "        similarity_weight = 0.3\n",
        "        \n",
        "        final_recommendations = book_details_recs.select(\n",
        "            books[\"ISBN\"],\n",
        "            books[\"Book-Title\"],\n",
        "            books[\"Book-Author\"],\n",
        "            recommendations_df[\"rating\"].alias(\"ALS_Rating\"),\n",
        "            similarities_df[\"Similarity\"],\n",
        "            (recommendations_df[\"rating\"] * als_weight + \n",
        "             similarities_df[\"Similarity\"] * similarity_weight).alias(\"Final_Score\")\n",
        "        ).orderBy(\"Final_Score\", ascending=False)\n",
        "        # Filter recommendations to only include those with similarity > 0.5\n",
        "        final_recommendations = final_recommendations.filter(\n",
        "            similarities_df[\"Similarity\"] > 0.3\n",
        "        )\n",
        "        return final_recommendations\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error getting recommendations: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = evaluate_recommendations(bestModel, training, k=10)\n",
        "print(f\"Precision@10: {metrics['precision@k']:.4f}\")\n",
        "f1_score = 2 * (metrics['precision@k'] * metrics['recall@k']) / (metrics['precision@k'] + metrics['recall@k']) if (metrics['precision@k'] + metrics['recall@k']) > 0 else 0\n",
        "print(f\"Recall@10: {metrics['recall@k']:.4f}\")\n",
        "print(f\"F1-Score@10: {f1_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import collect_list\n",
        "import numpy as np\n",
        "def calculate_map_ndcg(model, test_data, k=10):\n",
        "    # Get actual rated books for each user\n",
        "    user_rated_books = test_data.groupBy(\"User-ID\") \\\n",
        "        .agg(collect_list(\"bookId\").alias(\"rated_books\"))\n",
        "    \n",
        "    # Generate recommendations for all users\n",
        "    # Check if recommendations already exist\n",
        "    if 'recommendations' in locals():\n",
        "        pass\n",
        "    else:\n",
        "        # Get unique users from test data\n",
        "        users = test_data.select(\"User-ID\").distinct()\n",
        "        # Generate recommendations for all users\n",
        "        recommendations = model.recommendForUserSubset(users, k)\n",
        "    # Join recommendations with actual ratings\n",
        "    evaluation_data = recommendations.join(user_rated_books, \"User-ID\")\n",
        "    \n",
        "    def calculate_user_metrics(row):\n",
        "        # Extract book IDs and scores from recommendations\n",
        "        recommended = [(r.bookId, r.rating) for r in row.recommendations]\n",
        "        actual = set(row.rated_books)\n",
        "        \n",
        "        # Calculate MAP@K\n",
        "        ap = 0.0\n",
        "        hits = 0\n",
        "        for i, (book_id, _) in enumerate(recommended):\n",
        "            if book_id in actual:\n",
        "                hits += 1\n",
        "                ap += hits / (i + 1)\n",
        "        map_score = ap / len(actual) if len(actual) > 0 else 0\n",
        "        \n",
        "        # Calculate NDCG@K\n",
        "        # Create ideal ranking (all relevant items first)\n",
        "        ideal_ranking = [1.0] * len(actual) + [0.0] * (k - len(actual))\n",
        "        \n",
        "        # Calculate DCG\n",
        "        dcg = 0.0\n",
        "        for i, (book_id, _) in enumerate(recommended):\n",
        "            if book_id in actual:\n",
        "                dcg += 1.0 / np.log2(i + 2)  # log2(i+2) because i starts at 0\n",
        "        \n",
        "        # Calculate IDCG\n",
        "        idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(actual), k)))\n",
        "        \n",
        "        # Calculate NDCG\n",
        "        ndcg = dcg / idcg if idcg > 0 else 0\n",
        "        \n",
        "        return (map_score, ndcg)\n",
        "    \n",
        "    # Apply the metrics calculation\n",
        "    metrics = evaluation_data.rdd.map(calculate_user_metrics).collect()\n",
        "    \n",
        "    # Calculate average MAP and NDCG\n",
        "    avg_map = np.mean([m[0] for m in metrics])\n",
        "    avg_ndcg = np.mean([m[1] for m in metrics])\n",
        "    \n",
        "    return {\n",
        "        \"map@k\": avg_map,\n",
        "        \"ndcg@k\": avg_ndcg\n",
        "    }\n",
        "\n",
        "# Calculate and print MAP@K and NDCG@K\n",
        "map_ndcg_metrics = calculate_map_ndcg(bestModel, training, k=10)\n",
        "print(f\"MAP@10: {map_ndcg_metrics['map@k']:.4f}\")\n",
        "print(f\"NDCG@10: {map_ndcg_metrics['ndcg@k']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_user_ratings.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create mapping between numeric IDs and ISBNs\n",
        "# Assuming you have the original mapping from when you created the model\n",
        "fitted_isbn_indexer = isbn_indexer.fit(ratings_spark_df)\n",
        "book_id_to_isbn = fitted_isbn_indexer.labels\n",
        "\n",
        "# Create a reverse mapping (ISBN to numeric ID)\n",
        "isbn_to_id = {isbn: idx for idx, isbn in enumerate(book_id_to_isbn)}\n",
        "\n",
        "def get_reranked_recommendations(user_id, current_book_isbn, num_recommendations=10):\n",
        "    # Convert ISBN to numeric ID\n",
        "    current_book_id = isbn_to_id.get(current_book_isbn)\n",
        "    if current_book_id is None:\n",
        "        print(f\"Book ISBN {current_book_isbn} not found in the mapping\")\n",
        "        return None\n",
        "    print(f\"Current book ID: {current_book_id}\")\n",
        "    \n",
        "    # Step 1: Get initial ALS recommendations\n",
        "    try:\n",
        "        user_recs = bestModel.recommendForUserSubset(\n",
        "            spark.createDataFrame([(user_id,)], [\"User-ID\"]), \n",
        "            num_recommendations\n",
        "        )\n",
        "        user_recs = user_recs.collect()\n",
        "        \n",
        "        if not user_recs:\n",
        "            print(f\"No recommendations found for user {user_id}\")\n",
        "            return None\n",
        "        \n",
        "        # Get recommendations list\n",
        "        recommendations = user_recs[0].recommendations\n",
        "        \n",
        "        # Step 2: Get the current book's features\n",
        "        current_book_features = None\n",
        "        for item in bestModel.itemFactors.collect():\n",
        "            if item.id == current_book_id:\n",
        "                current_book_features = np.array(item.features)\n",
        "                break\n",
        "        if current_book_features is None:\n",
        "            print(f\"Current book ID {current_book_id} not found in the model\")\n",
        "            return None\n",
        "        \n",
        "        # Step 3: Calculate similarities and create a list of (ISBN, rating, similarity)\n",
        "        recommendations_with_similarity = []\n",
        "        for rec in recommendations:  # Use recommendations instead of user_recs[0].recommendations\n",
        "            rec_features = None\n",
        "            for item in bestModel.itemFactors.collect():\n",
        "                if item.id == rec.bookId:\n",
        "                    rec_features = np.array(item.features)\n",
        "                    break\n",
        "            \n",
        "            if rec_features is not None:\n",
        "                # Convert current book features to numpy array if not already\n",
        "                if not isinstance(current_book_features, np.ndarray):\n",
        "                    current_book_features = np.array(current_book_features)\n",
        "                \n",
        "                # Calculate cosine similarity using numpy\n",
        "                dot_product = np.dot(current_book_features, rec_features)\n",
        "                norm_current = np.linalg.norm(current_book_features)\n",
        "                norm_rec = np.linalg.norm(rec_features)\n",
        "                similarity = float(dot_product / (norm_current * norm_rec))\n",
        "                \n",
        "                # Convert numeric ID back to ISBN for display\n",
        "                isbn = book_id_to_isbn[rec.bookId]\n",
        "                recommendations_with_similarity.append((isbn, rec.rating, similarity))\n",
        "            else:\n",
        "                print(f\"Could not find features for bookId: {rec.bookId}\")\n",
        "        \n",
        "        if not recommendations_with_similarity:\n",
        "            print(\"No similar books found\")\n",
        "            return None\n",
        "        \n",
        "        # Step 4: Create DataFrames for recommendations and similarities\n",
        "        recommendations_df = spark.createDataFrame(\n",
        "            [(isbn, rating) for isbn, rating, _ in recommendations_with_similarity],\n",
        "            [\"ISBN\", \"rating\"]\n",
        "        )\n",
        "        \n",
        "        similarities_df = spark.createDataFrame(\n",
        "            [(isbn, similarity) for isbn, _, similarity in recommendations_with_similarity],\n",
        "            [\"ISBN\", \"Similarity\"]\n",
        "        )\n",
        "        \n",
        "        # Step 5: Join with book details and similarities\n",
        "        book_details_recs = recommendations_df.join(\n",
        "            books,\n",
        "            recommendations_df.ISBN == books.ISBN,\n",
        "            \"inner\"\n",
        "        ).join(\n",
        "            similarities_df,\n",
        "            recommendations_df.ISBN == similarities_df.ISBN,\n",
        "            \"inner\"\n",
        "        )\n",
        "        \n",
        "        # Step 6: Calculate final score (combine ALS rating and similarity)\n",
        "        als_weight = 0.7\n",
        "        similarity_weight = 0.3\n",
        "        \n",
        "        final_recommendations = book_details_recs.select(\n",
        "            books[\"ISBN\"],\n",
        "            books[\"Book-Title\"],\n",
        "            books[\"Book-Author\"],\n",
        "            recommendations_df[\"rating\"].alias(\"ALS_Rating\"),\n",
        "            similarities_df[\"Similarity\"],\n",
        "            (recommendations_df[\"rating\"] * als_weight + \n",
        "             similarities_df[\"Similarity\"] * similarity_weight).alias(\"Final_Score\")\n",
        "        ).orderBy(\"Final_Score\", ascending=False)\n",
        "        # Filter recommendations to only include those with similarity > 0.5\n",
        "        final_recommendations = final_recommendations.filter(\n",
        "            similarities_df[\"Similarity\"] > 0.3\n",
        "        )\n",
        "        return final_recommendations\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error getting recommendations: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get 50 random users and books for testing\n",
        "random_users = ratings_spark_df.select(\"User-ID\").distinct().limit(5)\n",
        "random_books = ratings_spark_df.select(\"ISBN\").distinct().limit(5)\n",
        "\n",
        "# Collect the users and books as lists\n",
        "user_list = [row[\"User-ID\"] for row in random_users.collect()]\n",
        "book_list = [row[\"ISBN\"] for row in random_books.collect()]\n",
        "\n",
        "print(\"Generating recommendations for 50 users and 50 books...\")\n",
        "\n",
        "# Initialize empty lists to store results\n",
        "all_recommendations = []\n",
        "\n",
        "# Generate recommendations for each user-book pair\n",
        "for user in user_list:\n",
        "    for book in book_list:\n",
        "        try:\n",
        "            # Get recommendations\n",
        "            recs = get_reranked_recommendations(user, book, num_recommendations=5)\n",
        "            if recs:\n",
        "                # Convert to pandas and add source user/book info\n",
        "                recs_pd = recs.toPandas()\n",
        "                recs_pd['Source_User'] = user\n",
        "                recs_pd['Source_Book'] = book\n",
        "                all_recommendations.append(recs_pd)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error getting recommendations for user {user}, book {book}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "print(\"\\nFinished generating recommendations\")\n",
        "\n",
        "# Combine all results\n",
        "if all_recommendations:\n",
        "    all_recs_df = pd.concat(all_recommendations, ignore_index=True)\n",
        "    print(\"\\nSample of recommendations:\")\n",
        "    print(all_recs_df.head())\n",
        "else:\n",
        "    print(\"No recommendations were generated successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(all_recs_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_recs_df.to_csv(\"reranked_recommendations.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "pickle.dump(all_recs_df, open('reranked_recommendations.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### With implicit data"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pcR-a0EyTARl",
        "YYV29B09Tz72",
        "IO_rAM6WVc-p"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
